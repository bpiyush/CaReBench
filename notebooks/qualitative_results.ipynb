{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2b931c-eca2-4a90-8177-8ac272441735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import einops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "import shared.utils as su\n",
    "from notebooks.eval_care_retrieval import load_model, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14797118-a26d-4c83-83f8-8510f7033542",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f8d21-c7d6-4ace-94fb-1d20e7a53aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b922e-8209-4797-84fb-8fad0552c95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c99bc-c387-4cd5-ad20-abe92023ea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f49a6-c071-4b66-b550-33188c71ac6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6469102d-108b-4431-ab77-35093016697b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9378972-2f95-47a8-b6d8-c3981db17e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7812f-ed17-44b5-b365-058e67800209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88e9b38-947f-49d4-96f0-ca01bf4e4b49",
   "metadata": {},
   "source": [
    "## Compute and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959b6065-1497-43ae-8f1a-1a9bbc361d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID:  /work/piyush/pretrained_checkpoints/CaRe-7B/\n",
      "Dataset:  charades\n",
      "\u001b[33mLoading CaRe model (/work/piyush/pretrained_checkpoints/CaRe-7B/)...............  \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EncoderForCaRe from /work/piyush/pretrained_checkpoints/CaRe-7B/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c2e2fd5a7e4384871e0e23d2e06e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of total parameters in Qwen2VLForConditionalGeneration: 8291.376M\n",
      "Number of rows:  5498\n",
      "Sample row: \n",
      "{\n",
      "    \"id_base\": \"YSKX3\",\n",
      "    \"subject\": \"CP6Y\",\n",
      "    \"scene\": \"Bedroom\",\n",
      "    \"quality\": 5.0,\n",
      "    \"relevance\": 6.0,\n",
      "    \"verified\": \"Yes\",\n",
      "    \"length\": 16.62,\n",
      "    \"cls_id\": \"c077\",\n",
      "    \"template\": \"putting a pillow somewhere\",\n",
      "    \"start_time\": 12.1,\n",
      "    \"end_time\": 16.62,\n",
      "    \"object_id\": \"o027\",\n",
      "    \"noun\": \"na\",\n",
      "    \"verb_id\": \"v016\",\n",
      "    \"verb\": \"put\",\n",
      "    \"label\": \"putting a pillow somewhere\",\n",
      "    \"id\": \"YSKX3_12.1_16.6\",\n",
      "    \"chiral_label\": 0.0,\n",
      "    \"chiral_triplet_id\": \"a9be73ec\",\n",
      "    \"text_id\": \"a9be73ec_0.0\",\n",
      "    \"video_path\": \"/scratch/shared/beegfs/piyush/datasets/Charades/Charades_v1_480_cut_clips/YSKX3_12.1_16.6.mp4\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a17fea6528a43438c1e1490738f0c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b312c99b05246b982343736ba021773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing video features:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /work/piyush/pretrained_checkpoints/CaRe-7B//metadata_results/charades_examples.pt.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    '/work/piyush/pretrained_checkpoints/CaRe-7B/',\n",
    "    # '/work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint',\n",
    "]\n",
    "datasets = [\n",
    "    # 'ssv2',\n",
    "    # 'epic',\n",
    "    'charades',\n",
    "]\n",
    "\n",
    "for model_id in models:\n",
    "    for dataset in datasets:\n",
    "        print(\"Model ID: \", model_id)\n",
    "        print(\"Dataset: \", dataset)\n",
    "\n",
    "        save_dir = f\"{model_id}/metadata_results\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = f\"{save_dir}/{dataset}_examples.pt\"\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"Results already exist for {model_id}/{dataset}. Skipping.\")\n",
    "            print('-' * 100)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # Load model\n",
    "            vfc, tfc, vp = load_model(_id=model_id, device_map='auto', n_frames=12)\n",
    "\n",
    "            # Load data\n",
    "            df = load_data(dataset)\n",
    "            df = df.drop_duplicates(subset=['id', 'text_id']).reset_index(drop=True)\n",
    "\n",
    "            # For each text query, only select 5 videos (for faster inference)\n",
    "            np.random.seed(42)\n",
    "            random.seed(42)\n",
    "            df_subset = []\n",
    "            n = 2\n",
    "            for text_id in df.text_id.unique():\n",
    "                subdf = df[df.text_id == text_id]\n",
    "                subdf = subdf.sample(n=min(len(subdf), n), random_state=42)\n",
    "                df_subset.append(subdf)\n",
    "            df = pd.concat(df_subset)\n",
    "\n",
    "            # Compute text features\n",
    "            text_ids = df['text_id'].unique()\n",
    "            texts_feat = {}\n",
    "            for text_id in su.log.tqdm_iterator(text_ids, desc='Computing text features'):\n",
    "                text = df[df.text_id == text_id].template.unique()[0]\n",
    "                with torch.no_grad():\n",
    "                    zt = tfc(text)\n",
    "                    zt = torch.nn.functional.normalize(zt, dim=-1)\n",
    "                texts_feat[text_id] = zt.cpu().float()\n",
    "\n",
    "            # Compute video features\n",
    "            video_paths = df.video_path.unique()\n",
    "            video_ids = df.id.unique()\n",
    "            video_feat = {}\n",
    "            is_qwen25vl = False\n",
    "            j = 0\n",
    "            for video_path in su.log.tqdm_iterator(video_paths, desc='Computing video features'):\n",
    "                if not is_qwen25vl:\n",
    "                    video_tensor = vp(video_path)\n",
    "                    with torch.no_grad():\n",
    "                        zv = vfc(video_tensor)\n",
    "                else:\n",
    "                    zv = vfc.encoder.encode_vision([video_path])[0]\n",
    "                zv = torch.nn.functional.normalize(zv, dim=-1)\n",
    "                video_feat[video_ids[j]] = zv.cpu().float()\n",
    "                j += 1\n",
    "\n",
    "            data = {\n",
    "                'video_embeddings': video_feat,\n",
    "                'text_embeddings': texts_feat,\n",
    "                'dataframe': df.copy(),\n",
    "            }\n",
    "            torch.save(data, save_path)\n",
    "            print(f\"Saved to {save_path}.\")\n",
    "            print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d60c17-f1a9-4058-979c-3374de735a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ae4ba-45c4-4c7c-b76c-64651fdbc271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e199f2be-d7f0-41a5-bbe3-63269a68a8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab68a6-a063-41b3-b9a7-aa4913ab039d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee06673-2087-4e74-beb7-2f26ada9e3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b2652-0646-442a-a3f4-11cadf286bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b1ba2f6-06e0-46b1-af49-de9571a6b4fc",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2966e9c5-b606-492b-9df2-3e1fa81fa7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLoading CaRe model (/work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint).  \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EncoderForTarsier from /work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint\n",
      "### do_image_padding is set as False, images will be resized directly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "TarsierForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da22c1a196304d9683f4e48e56beea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of total parameters in TarsierForConditionalGeneration: 7063.427M\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \n",
    "# model_id = '/work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint'\n",
    "vfc, tfc, vp = load_model(_id=model_id, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fae49-faee-4ccd-b3f3-5e944b514757",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ssv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2e4bcc-bc60-48e0-a547-df568f9b5310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1430\n",
      "Sample row: \n",
      "{\n",
      "    \"id\": 69703,\n",
      "    \"label\": \"moving pen up\",\n",
      "    \"template\": \"Moving [something] up\",\n",
      "    \"placeholders\": \"['pen']\",\n",
      "    \"target\": 114,\n",
      "    \"chiral_label\": 0.0,\n",
      "    \"chiral_triplet_id\": \"3f20f09b\",\n",
      "    \"noun\": \"['something']\",\n",
      "    \"text_id\": \"3f20f09b_0.0\",\n",
      "    \"video_path\": \"/scratch/shared/beegfs/piyush/datasets/SSv2/20bn-something-something-v2/69703.webm\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1430, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(dataset)\n",
    "df = df.drop_duplicates(subset=['id', 'text_id']).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "613c6b52-2b2f-4a42-a36f-06675a51ee11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each text query, only select 5 videos (for faster inference)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "df_subset = []\n",
    "n = 5\n",
    "for text_id in df.text_id.unique():\n",
    "    subdf = df[df.text_id == text_id]\n",
    "    subdf = subdf.sample(n=min(len(subdf), n), random_state=42)\n",
    "    df_subset.append(subdf)\n",
    "df = pd.concat(df_subset)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f245cbdb-0c6c-44df-8a13-2444c2f8a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d217cad75ec47a18a2d0b11e9d9abb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute text features\n",
    "text_ids = df['text_id'].unique()\n",
    "texts_feat = {}\n",
    "for text_id in su.log.tqdm_iterator(text_ids, desc='Computing text features'):\n",
    "    text = df[df.text_id == text_id].template.unique()[0]\n",
    "    zt = tfc(text)\n",
    "    zt = torch.nn.functional.normalize(zt, dim=-1)\n",
    "    texts_feat[text_id] = zt.cpu().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11910ed8-9953-486d-ac52-0293783be711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0617303ad96b4f418e3aaf720646def6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing video features:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute video features\n",
    "video_paths = df.video_path.unique()\n",
    "video_ids = df.id.unique()\n",
    "video_feat = {}\n",
    "is_qwen25vl = False\n",
    "j = 0\n",
    "for video_path in su.log.tqdm_iterator(video_paths, desc='Computing video features'):\n",
    "    if not is_qwen25vl:\n",
    "        video_tensor = vp(video_path)\n",
    "        zv = vfc(video_tensor)\n",
    "    else:\n",
    "        zv = vfc.encoder.encode_vision([video_path])[0]\n",
    "    zv = torch.nn.functional.normalize(zv, dim=-1)\n",
    "    video_feat[video_ids[j]] = zv.cpu().float()\n",
    "    j += 1\n",
    "len(video_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06ff54c5-8856-4d10-b718-4d0efda63d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint/metadata_results/ssv2_examples.pt.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "512\t/work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint/metadata_results/ssv2_examples.pt\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'video_embeddings': video_feat,\n",
    "    'text_embeddings': texts_feat,\n",
    "    'dataframe': df.copy(),\n",
    "}\n",
    "save_dir = f\"{model_id}/metadata_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = f\"{save_dir}/{dataset}_examples.pt\"\n",
    "torch.save(data, save_path)\n",
    "print(f\"Saved to {save_path}.\")\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd58478a-084e-4cc2-a2c2-edc8cafd2ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6M\t/work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint/metadata_results/ssv2_examples.pt\n"
     ]
    }
   ],
   "source": [
    "!du -sh $save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "935aa355-4926-4185-8ab0-a3d245400cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['video_embeddings', 'text_embeddings', 'dataframe']), 159)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.load(save_path)\n",
    "data.keys(), len(data['video_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e75cbd1-cc9e-46d2-afd7-e27753c17c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
