{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091e1d96-4b99-4c24-99c0-76775043c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import shared.utils as su"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1a416-bad3-4e28-8944-4641ad6e529d",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e46b2a-8303-44fc-9968-c1e5fd885133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"/scratch/shared/beegfs/piyush/datasets/SimCSE-NLI/nli-27k+ego4d-3k.csv\"\n",
    "df_train = pd.read_csv(csv_path)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873b5c9b-78d6-4210-aae0-c8f37509a279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sent0': 'Male street vendor selling an ear of corn.',\n",
       " 'sent1': 'The street vendor is outside.',\n",
       " 'hard_neg': 'The street vendor is under arrest.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205d4fe7-5cb5-44a2-bc20-ae2de0c63e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1430\n",
      "Sample row: \n",
      "{\n",
      "    \"id\": 69703,\n",
      "    \"label\": \"moving pen up\",\n",
      "    \"template\": \"Moving [something] up\",\n",
      "    \"placeholders\": \"['pen']\",\n",
      "    \"target\": 114,\n",
      "    \"chiral_label\": 0.0,\n",
      "    \"chiral_triplet_id\": \"3f20f09b\",\n",
      "    \"noun\": \"['something']\",\n",
      "    \"text_id\": \"3f20f09b_0.0\",\n",
      "    \"video_path\": \"/scratch/shared/beegfs/piyush/datasets/SSv2/20bn-something-something-v2/69703.webm\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1430, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebooks.eval_care import load_data\n",
    "df_valid = load_data(dataset='ssv2', split='validation')\n",
    "df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268a08e-64a4-4619-aa4f-02e47f0596ab",
   "metadata": {},
   "source": [
    "**Load model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "381ad3f4-4cee-43ca-8814-d830cb26c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling_encoders import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619f6c16-2922-4ab9-ab4b-ae05e28eaba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EncoderForCaRe from /work/piyush/pretrained_checkpoints/CaRe-7B-Stage-1/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4ee757e3e04d5ab62d596c00e781c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<models.modeling_encoders.EncoderForCaRe at 0x7f065febf2d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = AutoEncoder.from_pretrained(\n",
    "    \"/work/piyush/pretrained_checkpoints/CaRe-7B-Stage-1/\",\n",
    "    device_map='auto',\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation='flash_attention_2',\n",
    ")\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "427c8155-9b88-43e4-bc7a-d06f1a651392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of total parameters in Qwen2VLForConditionalGeneration: 8291.376M\n"
     ]
    }
   ],
   "source": [
    "su.misc.num_params(encoder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca53fe7-6f21-48a7-bce2-3502dc4afc84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 270, 480])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.video import read_frames_decord\n",
    "\n",
    "video_path = df_valid.video_path[0]\n",
    "n_frames = 16\n",
    "video = read_frames_decord(video_path, n_frames, width=480, height=270)\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec7fe82-32d0-4c84-a3d0-0384af0210ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3584]), torch.Size([1, 3584]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    zv = encoder.encode_vision(pixel_values=video.unsqueeze(0))\n",
    "    zt = encoder.encode_text(['A dog'])\n",
    "zv.shape, zt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc82465-3b4c-4384-b46c-89c48b37f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     zv = encoder.encode_vision(pixel_values=torch.stack([video] * 12)).cpu()\n",
    "# zv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d13e6f4a-fea8-439a-9cb0-e90d7339ef0a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a43a20c50b641429264afa501ee14ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing video features:   0%|          | 0/1430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m compute_metrics(df, video_feat, texts_feat, show_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[0;32m---> 33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mvalidation_metrics\u001b[0;34m(encoder, df)\u001b[0m\n\u001b[1;32m     12\u001b[0m video \u001b[38;5;241m=\u001b[39m read_frames_decord(video_path, n_frames, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m480\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m270\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     zv \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m zv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(zv, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m video_feat[video_ids[j]] \u001b[38;5;241m=\u001b[39m zv\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/projects/CaReBench/models/modeling_encoders.py:468\u001b[0m, in \u001b[0;36mEncoderForCaRe.encode_vision\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    466\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 468\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     vision_embs\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m    470\u001b[0m vision_embs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(vision_embs)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/generation/utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2530\u001b[0m         inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2535\u001b[0m     )\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2552\u001b[0m         input_ids,\n\u001b[1;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2558\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/generation/utils.py:2937\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2934\u001b[0m     streamer\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m:\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m GenerateEncoderDecoderOutput(\n\u001b[1;32m   2939\u001b[0m             sequences\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2940\u001b[0m             scores\u001b[38;5;241m=\u001b[39mscores,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             past_key_values\u001b[38;5;241m=\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2948\u001b[0m         )\n\u001b[1;32m   2949\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/configuration_utils.py:204\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    201\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(key, value)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    206\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from notebooks.eval_care_retrieval import compute_metrics\n",
    "\n",
    "\n",
    "def validation_metrics(encoder, df):\n",
    "\n",
    "    # Compute video metrics\n",
    "    video_paths = df.video_path.unique()\n",
    "    video_ids = df.id.unique()\n",
    "    video_feat = {}\n",
    "    j = 0\n",
    "    for video_path in su.log.tqdm_iterator(video_paths, desc='Computing video features'):\n",
    "        video = read_frames_decord(video_path, n_frames, width=480, height=270)\n",
    "        with torch.no_grad():\n",
    "            zv = encoder.encode_vision(pixel_values=video.unsqueeze(0)).cpu().squeeze(0)\n",
    "        zv = torch.nn.functional.normalize(zv, dim=-1)\n",
    "        video_feat[video_ids[j]] = zv.cpu().float()\n",
    "        j += 1\n",
    "\n",
    "    # Compute text features\n",
    "    text_ids = df['text_id'].unique()\n",
    "    texts_feat = {}\n",
    "    for text_id in su.log.tqdm_iterator(text_ids, desc='Computing text features'):\n",
    "        text = df[df.text_id == text_id].template.unique()[0]\n",
    "        with torch.no_grad():\n",
    "            zt = encoder.encode_text([text]).cpu().squeeze(0)\n",
    "        zt = torch.nn.functional.normalize(zt, dim=-1)\n",
    "        texts_feat[text_id] = zt.cpu().float()\n",
    "\n",
    "    metrics = compute_metrics(df, video_feat, texts_feat, show_metrics=False)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "metrics = validation_metrics(encoder, df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cb73bfc-3fe5-4013-8cf9-44d491bbe30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of trainable parameters in PeftModelForSeq2SeqLM: 2.523 M\n"
     ]
    }
   ],
   "source": [
    "su.misc.num_trainable_params(encoder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "facc73d9-b448-4fa5-ad0e-e9cf314202ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,293,898,752 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "encoder.model = get_peft_model(encoder.model, peft_config)\n",
    "encoder.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "627ebfa2-71cb-4f75-886c-7b6220f6eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of trainable parameters in PeftModelForCausalLM: 2.523 M\n"
     ]
    }
   ],
   "source": [
    "su.misc.num_trainable_params(encoder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf1be716-5d63-4079-bac8-0830745b6dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd9ee76e7224910a61ba71e2a34836c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing video features:   0%|          | 0/1430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mvalidation_metrics\u001b[0;34m(encoder, df)\u001b[0m\n\u001b[1;32m     12\u001b[0m video \u001b[38;5;241m=\u001b[39m read_frames_decord(video_path, n_frames, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m480\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m270\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     zv \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m zv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(zv, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m video_feat[video_ids[j]] \u001b[38;5;241m=\u001b[39m zv\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/projects/CaReBench/models/modeling_encoders.py:468\u001b[0m, in \u001b[0;36mEncoderForCaRe.encode_vision\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    466\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 468\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     vision_embs\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m    470\u001b[0m vision_embs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(vision_embs)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/peft/peft_model.py:1974\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1973\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1974\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/peft/peft_model.py:1704\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1703\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1704\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/generation/utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2530\u001b[0m         inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2535\u001b[0m     )\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2552\u001b[0m         input_ids,\n\u001b[1;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2558\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/generation/utils.py:2937\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2934\u001b[0m     streamer\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m:\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m GenerateEncoderDecoderOutput(\n\u001b[1;32m   2939\u001b[0m             sequences\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2940\u001b[0m             scores\u001b[38;5;241m=\u001b[39mscores,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             past_key_values\u001b[38;5;241m=\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2948\u001b[0m         )\n\u001b[1;32m   2949\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/configuration_utils.py:204\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    201\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(key, value)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    206\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = validation_metrics(encoder, df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528619b1-d6a9-4015-a1f4-441f521f746a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c9183-9b45-4998-971f-8299e70f345b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70095218-ac3c-4744-b7e2-792513fbf6db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Failed attempts at speeding up video feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5a4e93-3955-431a-87ea-d60b0bf61917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5aedba084f46c590fd13aa3e06e2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing video features:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 134\u001b[0m\n\u001b[1;32m    130\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m compute_metrics(df, video_feat, texts_feat, show_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[0;32m--> 134\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_metrics_faster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 92\u001b[0m, in \u001b[0;36mvalidation_metrics_faster\u001b[0;34m(encoder, df, n_frames, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     89\u001b[0m     video \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 92\u001b[0m     zv \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_vision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m zv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(zv, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m video_feat[video_id] \u001b[38;5;241m=\u001b[39m zv\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/projects/CaReBench/models/modeling_encoders.py:468\u001b[0m, in \u001b[0;36mEncoderForCaRe.encode_vision\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    466\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 468\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     vision_embs\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m    470\u001b[0m vision_embs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(vision_embs)\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/generation/utils.py:2539\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   2529\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2530\u001b[0m         inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2534\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2535\u001b[0m     )\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mSAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2540\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2544\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2547\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[1;32m   2551\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2552\u001b[0m         input_ids,\n\u001b[1;32m   2553\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2558\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/generation/utils.py:2937\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2934\u001b[0m     streamer\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m   2936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n\u001b[0;32m-> 2937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_encoder_decoder\u001b[49m:\n\u001b[1;32m   2938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m GenerateEncoderDecoderOutput(\n\u001b[1;32m   2939\u001b[0m             sequences\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2940\u001b[0m             scores\u001b[38;5;241m=\u001b[39mscores,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             past_key_values\u001b[38;5;241m=\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   2948\u001b[0m         )\n\u001b[1;32m   2949\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qwen/lib/python3.11/site-packages/transformers/configuration_utils.py:204\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    201\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(key, value)\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    206\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, video_ids, n_frames, width=480, height=270):\n",
    "        self.video_paths = video_paths\n",
    "        self.video_ids = video_ids\n",
    "        self.n_frames = n_frames\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        video_id = self.video_ids[idx]\n",
    "        \n",
    "        # Load video frames\n",
    "        video = read_frames_decord(video_path, self.n_frames, \n",
    "                                   width=self.width, height=self.height)\n",
    "        \n",
    "        return {\n",
    "            'video': video,\n",
    "            'video_id': video_id,\n",
    "            'video_path': video_path\n",
    "        }\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.text_ids = df['text_id'].unique()\n",
    "        self.texts = [df[df.text_id == tid].template.unique()[0] \n",
    "                      for tid in self.text_ids]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text_id': self.text_ids[idx],\n",
    "            'text': self.texts[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "def validation_metrics_faster(encoder, df, n_frames=16, batch_size=4, num_workers=4):\n",
    "    \"\"\"\n",
    "    Compute validation metrics with parallelized data loading.\n",
    "    \n",
    "    Args:\n",
    "        encoder: The encoder model\n",
    "        df: DataFrame containing video_path, id, text_id, template columns\n",
    "        n_frames: Number of frames to extract from each video\n",
    "        batch_size: Batch size for DataLoader (for loading, not model inference)\n",
    "        num_workers: Number of workers for parallel data loading\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary of computed metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # ==================== Compute Video Features ====================\n",
    "    video_paths = df.video_path.unique()\n",
    "    video_ids = df.id.unique()\n",
    "    \n",
    "    video_dataset = VideoDataset(video_paths, video_ids, n_frames, \n",
    "                                 width=480, height=270)\n",
    "    video_loader = DataLoader(\n",
    "        video_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster transfer to GPU\n",
    "        prefetch_factor=2  # Prefetch 2 batches per worker\n",
    "    )\n",
    "    \n",
    "    video_feat = {}\n",
    "    \n",
    "    for batch in su.log.tqdm_iterator(video_loader, desc='Computing video features'):\n",
    "        videos = batch['video']  # Shape: (batch_size, C, T, H, W)\n",
    "        batch_video_ids = batch['video_id']\n",
    "        \n",
    "        # Process each video in the batch individually to avoid OOM\n",
    "        for i in range(len(videos)):\n",
    "            video = videos[i].unsqueeze(0)  # Add batch dimension\n",
    "            video_id = batch_video_ids[i]\n",
    "            \n",
    "            # Move to GPU and compute features\n",
    "            if torch.cuda.is_available():\n",
    "                video = video.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                zv = encoder.encode_vision(pixel_values=video).cpu().squeeze(0)\n",
    "            \n",
    "            zv = torch.nn.functional.normalize(zv, dim=-1)\n",
    "            video_feat[video_id] = zv.float()\n",
    "            \n",
    "            # Clean up\n",
    "            del video\n",
    "        \n",
    "        # Clean up batch\n",
    "        del videos\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # ==================== Compute Text Features ====================\n",
    "    text_dataset = TextDataset(df)\n",
    "    text_loader = DataLoader(\n",
    "        text_dataset,\n",
    "        batch_size=32,  # Can use larger batch for text\n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    texts_feat = {}\n",
    "    \n",
    "    for batch in su.log.tqdm_iterator(text_loader, desc='Computing text features'):\n",
    "        batch_texts = batch['text']\n",
    "        batch_text_ids = batch['text_id']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            zt_batch = encoder.encode_text(list(batch_texts)).cpu()\n",
    "        \n",
    "        zt_batch = torch.nn.functional.normalize(zt_batch, dim=-1)\n",
    "        \n",
    "        # Store individually\n",
    "        for text_id, zt in zip(batch_text_ids, zt_batch):\n",
    "            texts_feat[text_id] = zt.float()\n",
    "    \n",
    "    # ==================== Compute Metrics ====================\n",
    "    metrics = compute_metrics(df, video_feat, texts_feat, show_metrics=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = validation_metrics_faster(encoder, df_valid, batch_size=8, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f5b92-bb6a-456e-993c-10c2bafa2139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76e4fb-dc86-4acb-8bc2-e6db47114691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
