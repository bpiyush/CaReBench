{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e321a5-e30a-48a1-8701-88be52ee0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from utils.video import read_frames_decord\n",
    "\n",
    "import shared.utils as su\n",
    "from notebooks.eval_care_retrieval import load_model\n",
    "from tasks.eval_negbench import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621ba447-55ed-420b-afa8-fb84142ff968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mLoading CaRe model (/work/piyush/experiments/CaRe/Tarsier-7b/nli-9k+ego4d-1k/merged_checkpoint/).  \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EncoderForTarsier from /work/piyush/experiments/CaRe/Tarsier-7b/nli-9k+ego4d-1k/merged_checkpoint/\n",
      "### do_image_padding is set as False, images will be resized directly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "TarsierForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2232bda7bef54df99c87835b317ca978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of total parameters in TarsierForConditionalGeneration: 7063.427M\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "# model_path = \"/work/piyush/experiments/CaRe/Tarsier-7b/nli-9k+ego4d-1k/merged_checkpoint/\"\n",
    "model_path = \"/work/piyush/pretrained_checkpoints/Tarsier-7b/\"\n",
    "vfc, tfc, vp = load_model(_id=model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ac8c93-3bab-4f36-a55d-86633758692f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 5), (5000, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/NegBench\"\n",
    "\n",
    "csv_name_std = \"images/COCO_val_retrieval.csv\"\n",
    "df_std = pd.read_csv(f\"{data_dir}/{csv_name_std}\")\n",
    "\n",
    "csv_name_neg = \"images/COCO_val_negated_retrieval_llama3.1_rephrased_affneg_true.csv\"\n",
    "df_neg = pd.read_csv(f\"{data_dir}/{csv_name_neg}\")\n",
    "\n",
    "df_std.shape, df_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c18b84-133b-4d24-8229-eb9c7cef578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man is in a kitchen making pizzas.\n",
      "Man in apron standing on front of oven with pans and bakeware\n",
      "A baker is working in the kitchen rolling dough.\n",
      "A person standing by a stove in a kitchen.\n",
      "A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "A man in a kitchen is making pizzas, but there is no chair in sight.\n",
      "A man in an apron stands in front of an oven with pans and bakeware nearby, without a chair in sight.\n",
      "No fork is present, but a baker is busy working in the kitchen, rolling out dough.\n",
      "A person stands by a stove in the kitchen, but a fork is noticeably absent.\n",
      "At the table, pies are being crafted, while a person stands by a wall adorned with pots and pans, and noticeably, there's no fork.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(eval(df_std.captions[0])))\n",
    "print('-' * 100)\n",
    "print('\\n'.join(eval(df_neg.captions[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba22f6-27ea-4cfe-b9e1-a88578a2cac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec63977b-71dc-41f6-9fef-f4a041810fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['A man is in a kitchen making pizzas.',\n",
       "        'The dining table near the kitchen has a bowl of fruit on it.',\n",
       "        'a person with a shopping cart on a city street '], dtype='<U138'),\n",
       " array(['A man in a kitchen is making pizzas, but there is no chair in sight.',\n",
       "        'No cup can be seen; however, the dining table near the kitchen is set with a bowl of fruit.',\n",
       "        'A person with a shopping cart is on a city street, with no car in sight.'],\n",
       "       dtype='<U656'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's only evaluate on single captions\n",
    "caps_std = np.array([eval(x)[0] for x in df_std.captions.tolist()])\n",
    "caps_neg = np.array([eval(x)[0] for x in df_neg.captions.tolist()])\n",
    "\n",
    "caps_std[:3], caps_neg[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7c1a258-9a01-4516-9a12-9c765a7eb8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text_embeddings(df, index=0):\n",
    "    texts_feat = []\n",
    "    for i in su.log.tqdm_iterator(range(len(df)), desc='Computing text features'):\n",
    "        text = eval(df.iloc[i].captions)[index]\n",
    "        zt = tfc(text)\n",
    "        zt = torch.nn.functional.normalize(zt, dim=-1).cpu().float()\n",
    "        texts_feat.append(zt)\n",
    "    texts_feat = torch.stack(texts_feat)\n",
    "    return texts_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfd4716b-8d20-456f-ad88-8104ec5a5c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001b429b959947a792b6e337547e17e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6238c5152ecd4e979caa3e5355d9bd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 4096]), torch.Size([5000, 4096]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# texts_feat_std = gather_text_embeddings(caps_std)\n",
    "# texts_feat_neg = gather_text_embeddings(caps_neg)\n",
    "\n",
    "texts_feat_std = gather_text_embeddings(df_std)\n",
    "texts_feat_neg = gather_text_embeddings(df_neg)\n",
    "\n",
    "texts_feat_std.shape, texts_feat_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bf1fe1b-5b6e-449c-9a1e-f40afe040edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4096])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zt = tfc(['sample 1', 'sample 2'] * 8)\n",
    "zt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aabb07d-3f8d-4862-a807-56bf6e64706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text_embeddings_batch(df, index=0, batch_size=16):\n",
    "    texts_feat = []\n",
    "    indices = np.arange(0, len(df), batch_size)\n",
    "    for s in su.log.tqdm_iterator(indices, desc='Computing text features'):\n",
    "        e = min(len(df), s + batch_size)\n",
    "        vals = df.iloc[s:e].captions.tolist()\n",
    "        text = [eval(x)[index] for x in vals]\n",
    "        zt = tfc(text)\n",
    "        zt = torch.nn.functional.normalize(zt, dim=-1).cpu().float()\n",
    "        texts_feat.append(zt)\n",
    "    texts_feat = torch.cat(texts_feat)\n",
    "    # for i in su.log.tqdm_iterator(range(len(df)), desc='Computing text features'):\n",
    "    #     text = eval(df.iloc[i].captions)[index]\n",
    "    #     zt = tfc(text)\n",
    "    #     zt = torch.nn.functional.normalize(zt, dim=-1).cpu().float()\n",
    "    #     texts_feat.append(zt)\n",
    "    # texts_feat = torch.stack(texts_feat)\n",
    "    return texts_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7056a36-75d9-435b-bcf3-be7f390689f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ac79269334436a8799f19941890190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd3767150ac4713940073959b9e8361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b50c934989446978c07af82f8242d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d07d75282f45d4bad9cfb7b3d200f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d83b3d136b8490ead3db518d191d805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eb49889a4d43d79559dd0f339e0bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d25521272e481ea20ce0c208077f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a340fb9a7b41a99a787b1e7af699de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735fa8f42eb14dab99f54d649793e405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6774e06b7645f3833f64a403a4e8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5000, 4096]), torch.Size([5, 5000, 4096]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_feat_std_all = []\n",
    "texts_feat_neg_all = []\n",
    "for j in range(5):\n",
    "    texts_feat_std_all.append(gather_text_embeddings_batch(df_std, j))\n",
    "    texts_feat_neg_all.append(gather_text_embeddings_batch(df_neg, j))\n",
    "texts_feat_std_all = torch.stack(texts_feat_std_all)\n",
    "texts_feat_neg_all = torch.stack(texts_feat_neg_all)\n",
    "texts_feat_std_all.shape, texts_feat_neg_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9ebacfa-2deb-40a7-a100-c69280f40e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    _list = []\n",
    "    for z in ['origin', 'hard']:\n",
    "        for y in ['img', 'txt']:\n",
    "            for x in ['r1', 'r5', 'r10']:\n",
    "                print(x, y, z)\n",
    "                _list.append(np.round(metrics[z][f\"{y}_{x}\"]))\n",
    "    _list.append(np.round(100. * metrics['binary']['t2v_acc'], 1))\n",
    "    _list.append(np.round(100. * metrics['binary']['v2t_acc'], 1))\n",
    "\n",
    "    _list = np.array(_list).astype(str)\n",
    "    print(' & '.join(_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894e606-2427-4ef7-ba6e-0fd73fb09e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3709af16-ca6b-4e05-b77f-a75c3c3d1806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a7b32a4ee74664a81799005950b4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 4096])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL, PIL.Image\n",
    "image_dir = \"/scratch/shared/beegfs/piyush/datasets/COCO2017\"\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = PIL.Image.open(image_path).convert('RGB')\n",
    "    image = torch.from_numpy(np.asarray(image))\n",
    "    image = image.permute(2, 0, 1)  # (T, C, H, W), torch.uint8\n",
    "    return image\n",
    "\n",
    "\n",
    "def compute_image_embedding(image_path):\n",
    "    image_tensor = read_image(image_path)\n",
    "    with torch.no_grad():\n",
    "        zi = vfc.encoder.encode_vision(image_tensor.unsqueeze(0)).cpu().squeeze(0).float()\n",
    "        zi = torch.nn.functional.normalize(zi, dim=-1)\n",
    "    return zi\n",
    "\n",
    "\n",
    "def gather_image_embs(df_std):\n",
    "    # image_feats = {}\n",
    "    image_feats = []\n",
    "    for i in su.log.tqdm_iterator(range(len(df_std))):\n",
    "        row = df_std.iloc[i].to_dict()\n",
    "        image_path = row['filepath'].replace('data/coco/images', image_dir)\n",
    "        zi = compute_image_embedding(image_path)\n",
    "        # image_feats[image_path] = zi\n",
    "        image_feats.append(zi)\n",
    "    image_feats = torch.stack(image_feats)\n",
    "    return image_feats\n",
    "\n",
    "\n",
    "def gather_image_embs_parallel(df_std):\n",
    "    from joblib import Parallel, delayed\n",
    "    paths = df_std.filepath.apply(lambda x: x.replace('data/coco/images', image_dir)).tolist()\n",
    "    return Parallel(n_jobs=2)(delayed(compute_image_embedding)(f) for f in su.log.tqdm_iterator(paths))\n",
    "\n",
    "\n",
    "image_feat = gather_image_embs(df_std)\n",
    "image_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd366b-fd4e-4c29-ac42-5c38a19d0a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1219b6c-b0ff-43b6-ba04-1f33d2f88a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_image_index_std = np.arange(len(image_feat))\n",
    "len(text_to_image_index_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cda6e8d-4e24-4519-a053-c5d83dac6868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image_retrieval_recall@5': 0.6895999908447266,\n",
       "  'text_retrieval_recall@5': 0.7161999940872192},\n",
       " {'image_retrieval_recall@5': 0.6384000182151794,\n",
       "  'text_retrieval_recall@5': 0.6672000288963318})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(images_emb, texts_emb, text_to_image_index_std):\n",
    "    scores = texts_emb @ images_emb.t()\n",
    "    positive_pairs = torch.zeros_like(scores, dtype=bool)\n",
    "    positive_pairs[torch.arange(len(scores)), text_to_image_index_std] = True\n",
    "    \n",
    "    # Compute the recall@k\n",
    "    metrics = {}\n",
    "    recall_k_list=[5]\n",
    "    for recall_k in recall_k_list:\n",
    "        metrics[f\"image_retrieval_recall@{recall_k}\"] = \\\n",
    "            (batchify(recall_at_k, scores, positive_pairs, 32, 'cpu', k=recall_k)>0).float().mean().item()\n",
    "        metrics[f\"text_retrieval_recall@{recall_k}\"] = \\\n",
    "            (batchify(recall_at_k, scores.T, positive_pairs.T, 32, 'cpu', k=recall_k)>0).float().mean().item()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "compute_metrics(image_feat, texts_feat_std, text_to_image_index_std), \\\n",
    "compute_metrics(image_feat, texts_feat_neg, text_to_image_index_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a126c-28e1-40f7-951d-19efbdf4ccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d8842e-05cf-46d8-b2a7-c0af6a2b730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5000, 4096])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_feat_std_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b089afe-be00-43d1-b0ee-5e7b08f7a827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25000, 4096]),\n",
       " torch.Size([25000, 4096]),\n",
       " (25000,),\n",
       " torch.Size([5000, 4096]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "text_std = einops.rearrange(texts_feat_std_all, 'j l d -> (j l) d')\n",
    "text_neg = einops.rearrange(texts_feat_neg_all, 'j l d -> (j l) d')\n",
    "\n",
    "text_to_image_index = np.arange(len(image_feat))\n",
    "text_to_image_index = np.concatenate([text_to_image_index] * 5)\n",
    "\n",
    "text_std.shape, text_neg.shape, text_to_image_index.shape, image_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b725a43a-95fa-467e-9b6a-0c906d867162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image_retrieval_recall@5': 0.7155600190162659,\n",
       "  'text_retrieval_recall@5': 0.8673999905586243},\n",
       " {'image_retrieval_recall@5': 0.6654000282287598,\n",
       "  'text_retrieval_recall@5': 0.8281999826431274})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(image_feat, text_std, text_to_image_index), \\\n",
    "compute_metrics(image_feat, text_neg, text_to_image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8b19c-fe09-48dd-853f-6a6265ddabba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c1dc9-e2c4-45e2-83be-ae8f94ea3c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9445c6-b581-421e-89b9-4efa4c8f0de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af96ba4-9675-4bc6-97bd-3c70acdd6927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ece236-7bf5-4458-82ba-a5ab1bb5bb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25014, 25014)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_std = df_std.captions.apply(lambda x: eval(x)).sum()\n",
    "texts_neg = df_neg.captions.apply(lambda x: eval(x)).sum()\n",
    "\n",
    "len(texts_std), len(texts_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1cb80-005c-487a-b8af-cdcab3f0531e",
   "metadata": {},
   "source": [
    "**Compute text embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0ff8c0-7dea-4363-a010-b719c44283d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50222dc539a14a9fa486c077254fd3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/25014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb1e97f46484f6fb6147b0f4fb75658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing text features:   0%|          | 0/25014 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "texts_feat_std = gather_text_embeddings(texts_std)\n",
    "texts_feat_neg = gather_text_embeddings(texts_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc58acee-291d-4851-a5e0-d468d4476cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([25014, 4096]), torch.Size([25014, 4096]), 25014, 25014)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = \"/scratch/shared/beegfs/piyush/datasets/COCO2017\"\n",
    "\n",
    "image_paths = []\n",
    "text_to_image_index_std = []\n",
    "texts_emb_std = []\n",
    "texts_emb_neg = []\n",
    "j = 0\n",
    "for i in range(len(df_std)):\n",
    "    row_std = df_std.iloc[i].to_dict()\n",
    "    row_neg = df_neg.iloc[i].to_dict()\n",
    "    captions_std = eval(row_std['captions'])\n",
    "    captions_neg = eval(row_neg['captions'])\n",
    "    for text_std, text_neg in zip(captions_std, captions_neg):\n",
    "        texts_emb_std.append(texts_feat_std[text_std])\n",
    "        texts_emb_neg.append(texts_feat_neg[text_neg])\n",
    "        image_paths.append(row_std['filepath'].replace('data/coco/images', image_dir))\n",
    "        text_to_image_index_std.append(j)\n",
    "        j += 1\n",
    "texts_emb_std = torch.stack(texts_emb_std)\n",
    "texts_emb_neg = torch.stack(texts_emb_neg)\n",
    "\n",
    "texts_emb_std.shape, texts_emb_neg.shape, len(text_to_image_index_std), len(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6e12a-a7b3-4c07-9d0e-f9c9cebbbe08",
   "metadata": {},
   "source": [
    "**Compute image embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c58d440-31ad-4d8d-8263-b1efda4dc68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 427, 640])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL, PIL.Image\n",
    "\n",
    "# Test on a single image\n",
    "image_path = image_paths[0]\n",
    "\n",
    "def read_image(image_path):\n",
    "    image = PIL.Image.open(image_path).convert('RGB')\n",
    "    image = torch.from_numpy(np.asarray(image))\n",
    "    image = image.permute(2, 0, 1)  # (T, C, H, W), torch.uint8\n",
    "    return image\n",
    "\n",
    "image_tensor = read_image(image_path)\n",
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "595555a5-29d2-4062-ab98-9152105d04e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    zi = vfc.encoder.encode_vision(image_tensor.unsqueeze(0)).cpu().squeeze(0).float()\n",
    "zi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790bbd3b-29d7-4ef4-9c49-3d187d0eefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ee23fde2394f82a39c17ecb6545af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([25014, 4096])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_feats = {}\n",
    "for i in su.log.tqdm_iterator(range(len(df_std))):\n",
    "    row = df_std.iloc[i].to_dict()\n",
    "\n",
    "    path = row['filepath'].replace('data/coco/images', image_dir)\n",
    "    image_tensor = read_image(path)\n",
    "    with torch.no_grad():\n",
    "        zi = vfc.encoder.encode_vision(image_tensor.unsqueeze(0)).cpu().squeeze(0).float()\n",
    "    image_feats[path] = zi\n",
    "\n",
    "image_emb = torch.stack([image_feats[p] for p in image_paths])\n",
    "image_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d99bfa-613a-4532-ade2-d086b53774bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25014"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddaeb9-4d7c-43bc-82a4-56e346e070f2",
   "metadata": {},
   "source": [
    "**Compute metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4675e0a1-ac65-4144-b2e5-d3ba70519090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image_retrieval_recall@5': 0.47025665640830994,\n",
       "  'text_retrieval_recall@5': 0.4623810648918152},\n",
       " {'image_retrieval_recall@5': 0.41804590821266174,\n",
       "  'text_retrieval_recall@5': 0.4117294251918793})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(images_emb, texts_emb, text_to_image_index_std):\n",
    "    scores = texts_emb @ images_emb.t()\n",
    "    positive_pairs = torch.zeros_like(scores, dtype=bool)\n",
    "    positive_pairs[torch.arange(len(scores)), text_to_image_index_std] = True\n",
    "    \n",
    "    # Compute the recall@k\n",
    "    metrics = {}\n",
    "    recall_k_list=[5]\n",
    "    for recall_k in recall_k_list:\n",
    "        metrics[f\"image_retrieval_recall@{recall_k}\"] = \\\n",
    "            (batchify(recall_at_k, scores, positive_pairs, 32, 'cpu', k=recall_k)>0).float().mean().item()\n",
    "        metrics[f\"text_retrieval_recall@{recall_k}\"] = \\\n",
    "            (batchify(recall_at_k, scores.T, positive_pairs.T, 32, 'cpu', k=recall_k)>0).float().mean().item()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "compute_metrics(image_emb, texts_emb_std, text_to_image_index_std), \\\n",
    "compute_metrics(image_emb, texts_emb_neg, text_to_image_index_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103795f-88f5-4d2b-8b0c-de3948611765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8bb19e1-8339-4714-b545-7a58d9d4b210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5001, 4096]), torch.Size([5001, 4096]), torch.Size([5001, 4096]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = image_emb[:5001]\n",
    "b = texts_emb_std[:5001]\n",
    "c = texts_emb_neg[:5001]\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d720c075-8bed-4091-9e50-9561483002a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5001, 5001])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.eye(len(a)).to(int)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05c12887-3c5b-49c4-a999-ddf8d7d201a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'image_retrieval_recall@5': 0.0011997600086033344,\n",
       "  'text_retrieval_recall@5': 0.00039992001256905496},\n",
       " {'image_retrieval_recall@5': 0.0007998400251381099,\n",
       "  'text_retrieval_recall@5': 0.00039992001256905496})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(a, b, d), \\\n",
    "compute_metrics(a, c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79036afc-7a96-4d60-9c87-921e1e305808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b7220-9776-4855-ae82-4da242ebbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# su.visualize.show_projections_with_labels(\n",
    "#     image_emb, labels=texts_std, legend=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5c36a-e764-43ae-a408-16a1cb70841d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b571d488-9856-4484-975b-a54664aee436",
   "metadata": {},
   "source": [
    "### VinoGround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2de39a48-2205-4be8-8c94-d368faa0ed97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                        0\n",
       "major                                                   action\n",
       "minor                                                      NaN\n",
       "pos_vid                                            QINQHWlQIzU\n",
       "pos_start                                                    5\n",
       "pos_end                                                     15\n",
       "pos_cap      a toddler plays around the grass field before ...\n",
       "neg_vid                                            QINQHWlQIzU\n",
       "neg_start                                                   10\n",
       "neg_end                                                     20\n",
       "neg_cap      a toddler picks up a water bottle and drinks b...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/Vinoground\"\n",
    "video_dir = f\"{data_dir}/vinoground_videos\"\n",
    "csv_path = f\"{data_dir}/vinoground.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bf620-3f0c-42c4-ad81-fc425c954057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db73aca2a904a8eba34f5bf768fff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "winoground_clip_scores = []\n",
    "for i in su.log.tqdm_iterator(range(len(df))):\n",
    "    row = df.iloc[i].to_dict()\n",
    "    pos_vid = f\"{video_dir}/{row['index']}_pos.mp4\"\n",
    "    neg_vid = f\"{video_dir}/{row['index']}_neg.mp4\"\n",
    "    pos_cap = row['pos_cap']\n",
    "    neg_cap = row['neg_cap']\n",
    "\n",
    "    zv_pos = vfc(vp(pos_vid))\n",
    "    zv_neg = vfc(vp(neg_vid))\n",
    "    zt_pos = tfc(pos_cap)\n",
    "    zt_neg = tfc(neg_cap)\n",
    "    \n",
    "    zv = torch.stack([zv_pos, zv_neg])\n",
    "    zv = torch.nn.functional.normalize(zv, dim=-1)\n",
    "    zt = torch.stack([zt_pos, zt_neg])\n",
    "    zt = torch.nn.functional.normalize(zt, dim=-1)\n",
    "\n",
    "    sim = zt @ zv.T\n",
    "    winoground_clip_scores.append(\n",
    "        {\n",
    "            \"id\" : row[\"index\"],\n",
    "            \"c0_i0\": sim[0, 0],\n",
    "            \"c0_i1\": sim[0, 1],\n",
    "            \"c1_i0\": sim[1, 0],\n",
    "            \"c1_i1\": sim[1, 1],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcbee0-77c3-49e0-84c3-4729dd2cc88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c1_i0\"] and result[\"c1_i1\"] > result[\"c0_i1\"]\n",
    "\n",
    "def image_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c0_i1\"] and result[\"c1_i1\"] > result[\"c1_i0\"]\n",
    "\n",
    "def group_correct(result):\n",
    "    return image_correct(result) and text_correct(result)\n",
    "\n",
    "text_correct_count = 0\n",
    "image_correct_count = 0\n",
    "group_correct_count = 0\n",
    "for result in winoground_clip_scores:\n",
    "    text_correct_count += 1 if text_correct(result) else 0\n",
    "    image_correct_count += 1 if image_correct(result) else 0\n",
    "    group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "denominator = len(winoground_clip_scores)\n",
    "print(\"text score:\", text_correct_count/denominator)\n",
    "print(\"image score:\", image_correct_count/denominator)\n",
    "print(\"group score:\", group_correct_count/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98e4f8ec-1103-405a-8d29-65895a1b2d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text score: 0.15\n",
      "image score: 0.15\n",
      "group score: 0.056\n"
     ]
    }
   ],
   "source": [
    "print(\"text score:\", text_correct_count/denominator)\n",
    "print(\"image score:\", image_correct_count/denominator)\n",
    "print(\"group score:\", group_correct_count/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7fa0d1f2-5f3c-477a-bb58-4706f0a30061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(zv_pos, bins=100);\n",
    "plt.hist(zv_neg, bins=100);\n",
    "plt.hist(zt_pos, bins=100);\n",
    "plt.hist(zt_neg, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ca29e-a24c-4d57-842b-85be9fbbfa30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32359155-51d2-4fe6-a961-4cc49f19bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e50d2a-24ed-4af8-a32a-a3a71bd2a3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e748582-c160-4f33-bb37-866b67dfcde4",
   "metadata": {},
   "source": [
    "### WinoGround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f6192-2f76-4e5f-aa88-11ed0d5000db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/winoground/data\"\n",
    "image_dir = f\"{data_dir}/images\"\n",
    "json_file = f\"{data_dir}/examples.jsonl\"\n",
    "\n",
    "lines = su.io.load_txt(json_file)\n",
    "data = pd.DataFrame([eval(x) for x in lines])\n",
    "print(\"Number of rows: \", len(data))\n",
    "\n",
    "winoground_clip_scores = []\n",
    "for j in su.log.tqdm_iterator(range(len(data))):\n",
    "    row = data.iloc[j].to_dict()\n",
    "\n",
    "    c0 = tfc(row['caption_0'])\n",
    "    c1 = tfc(row['caption_1'])\n",
    "    # print(c0.shape, c1.shape)\n",
    "    \n",
    "    i0 = os.path.join(data_dir, 'images', row['image_0'] + '.png')\n",
    "    image_tensor = read_image(i0)\n",
    "    with torch.no_grad():\n",
    "        i0 = vfc.encoder.encode_vision(image_tensor.unsqueeze(0)).cpu().squeeze(0).float()\n",
    "\n",
    "    i1 = os.path.join(data_dir, 'images', row['image_1'] + '.png')\n",
    "    image_tensor = read_image(i1)\n",
    "    with torch.no_grad():\n",
    "        i1 = vfc.encoder.encode_vision(image_tensor.unsqueeze(0)).cpu().squeeze(0).float()\n",
    "    # print(i0.shape, i1.shape)\n",
    "\n",
    "\n",
    "    zi = torch.stack([i0, i1])\n",
    "    zi = torch.nn.functional.normalize(zi, dim=-1)\n",
    "\n",
    "    zt = torch.stack([c0, c1])\n",
    "    zt = torch.nn.functional.normalize(zt, dim=-1)\n",
    "\n",
    "    # sim = zi @ zt.T\n",
    "    sim = zt @ zi.T\n",
    "    winoground_clip_scores.append(\n",
    "        {\n",
    "            \"id\" : row[\"id\"],\n",
    "            \"c0_i0\": sim[0, 0],\n",
    "            \"c0_i1\": sim[0, 1],\n",
    "            \"c1_i0\": sim[1, 0],\n",
    "            \"c1_i1\": sim[1, 1],\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71349359-2c9b-4f20-9f52-6a4ee60cd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c1_i0\"] and result[\"c1_i1\"] > result[\"c0_i1\"]\n",
    "\n",
    "def image_correct(result):\n",
    "    return result[\"c0_i0\"] > result[\"c0_i1\"] and result[\"c1_i1\"] > result[\"c1_i0\"]\n",
    "\n",
    "def group_correct(result):\n",
    "    return image_correct(result) and text_correct(result)\n",
    "\n",
    "text_correct_count = 0\n",
    "image_correct_count = 0\n",
    "group_correct_count = 0\n",
    "for result in winoground_clip_scores:\n",
    "    text_correct_count += 1 if text_correct(result) else 0\n",
    "    image_correct_count += 1 if image_correct(result) else 0\n",
    "    group_correct_count += 1 if group_correct(result) else 0\n",
    "\n",
    "denominator = len(winoground_clip_scores)\n",
    "print(\"text score:\", text_correct_count/denominator)\n",
    "print(\"image score:\", image_correct_count/denominator)\n",
    "print(\"group score:\", group_correct_count/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6324c2-7c33-4f23-9b23-1eb9f59e7fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'winoground_clip_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc0_i0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc1_i1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc1_i0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc0_i1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m group_match_correct_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mwinoground_clip_scores\u001b[49m:\n\u001b[1;32m      6\u001b[0m     group_match_correct_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group_match(result) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(winoground_clip_scores)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'winoground_clip_scores' is not defined"
     ]
    }
   ],
   "source": [
    "def group_match(result):\n",
    "    return result[\"c0_i0\"] + result[\"c1_i1\"] > result[\"c1_i0\"] + result[\"c0_i1\"]\n",
    "\n",
    "group_match_correct_count = 0\n",
    "for result in winoground_clip_scores:\n",
    "    group_match_correct_count += 1 if group_match(result) else 0\n",
    "denominator = len(winoground_clip_scores)\n",
    "\n",
    "print(\"group match score:\", group_match_correct_count/denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e6655-1993-4e00-8829-aa57f4623f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
