{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb1dcae-c6e0-4fa1-b586-d4363f2edf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from utils.video import read_frames_decord\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shared.utils as su\n",
    "from utils.video import read_frames_decord\n",
    "from utils.model import transform_pixel_values\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToPILImage,\n",
    ")\n",
    "# from notebooks.eval_care_retrieval import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872fec5-a386-4746-a1dd-76f5e9b01c18",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99cc0ebc-8b5c-4b67-b070-59cec872daba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EncoderForTarsier from /work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint\n",
      "### do_image_padding is set as False, images will be resized directly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "TarsierForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4559f1963dc1460d941ce0ef73ab625a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of total parameters in TarsierForConditionalGeneration: 7063.427M\n"
     ]
    }
   ],
   "source": [
    "from models.modeling_encoders import AutoEncoder\n",
    "\n",
    "# model_id = \"/work/piyush/experiments/CaRe/Tarsier-7b/nli-9k+ego4d-1k/merged_checkpoint\"\n",
    "model_id = \"/work/piyush/experiments/CaRe/Tarsier-7b/final-10112025/nli_9000+ego_1000+subj_replaced-seed_42/merged_checkpoint\"\n",
    "# !ls $model_id\n",
    "\n",
    "encoder = AutoEncoder.from_pretrained(model_id, device_map='auto')\n",
    "su.misc.num_params(encoder.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d95b29-3c03-4efe-94b8-90f9287d0e91",
   "metadata": {},
   "source": [
    "### TVBench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ba9cc1-1356-451f-a0f0-61addaeb3bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file: /scratch/shared/beegfs/piyush/datasets/TVBench/all_except_ntu120vids.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2405, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/TVBench\"\n",
    "video_dir = f\"{data_dir}/video\"\n",
    "csv_path = f\"{data_dir}/all_except_ntu120vids.csv\"\n",
    "print(f\"CSV file: {csv_path}\")\n",
    "assert os.path.exists(csv_path), f\"CSV file not found: {csv_path}\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23dbf26f-8c94-47de-bb9e-c78bc0e9271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/scratch/shared/beegfs/piyush/datasets/TVBench\"\n",
    "data_dir = f\"{data_root}/json\"\n",
    "data_list = {\n",
    "    \"Action Count\": (\"action_count.json\", f\"{data_root}/video/action_count\", \"video\", False),\n",
    "    \"Object Count\": (\"object_count.json\", f\"{data_root}/video/object_count\", \"video\", False),\n",
    "    \"Action Sequence\": (\"action_sequence.json\", f\"{data_root}/video/action_sequence\", \"video\", True),  # has start & end\n",
    "    \"Object Shuffle\": (\"object_shuffle.json\", f\"{data_root}/video/object_shuffle\", \"video\", False),\n",
    "    \"Scene Transition\": (\"scene_transition.json\", f\"{data_root}/video/scene_transition\", \"video\", False),\n",
    "    \"Action Localization\": (\"action_localization.json\", f\"{data_root}/video/action_localization\", \"video\", True),  # has start & end\n",
    "    \"Action Antonym\": (\"action_antonym.json\", f\"{data_root}/video/action_antonym\", \"video\", False),\n",
    "    \"Unexpected Action\": (\"unexpected_action.json\", f\"{data_root}/video/unexpected_action\", \"video\", False),\n",
    "    \"Egocentric Sequence\": (\"egocentric_sequence.json\", f\"{data_root}/video/egocentric_sequence\", \"video\", False),\n",
    "    \"Moving Direction\": (\"moving_direction.json\", f\"{data_root}/video/moving_direction\", \"video\", False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38950b9d-8c41-48bf-9941-33fa2c06479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd8d78ea35f481497c9e53b0ca3ac67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Action Count::   0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cd0d0e2ee041778d9f4d29e3a85923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Object Count::   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c2e9734af5405cab2cfeb5c6ebfb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Action Sequence::   0%|          | 0/437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df47ed02df3497797975794ed760b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Object Shuffle::   0%|          | 0/225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c942a997fc47436b829a86899911a0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Scene Transition::   0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a02c75e7424f789f9f32b09f80fbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Action Localization::   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62cd749381846948ec8bbede6af9c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Action Antonym::   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc60560f1eca402ea430bf2e6724af55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Unexpected Action::   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a13e75bab854b6eaf4df06fbbdf67ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Egocentric Sequence::   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19dd4ffdc3841f69bc1f52ad4e90f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering video for Moving Direction::   0%|          | 0/232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2405, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "res_list = []\n",
    "acc_dict = {}\n",
    "\n",
    "# df = {\n",
    "#     'video_path': [],\n",
    "#     'start_time': [],\n",
    "#     'end_time': [],\n",
    "#     'question': [],\n",
    "#     'candidates': [],\n",
    "#     'answer': [],\n",
    "# }\n",
    "\n",
    "df = []\n",
    "\n",
    "for key in data_list:\n",
    "    config = data_list[key]\n",
    "\n",
    "    # Load JSON\n",
    "    json_path = f\"{data_dir}/{config[0]}\"\n",
    "    assert os.path.exists(json_path)\n",
    "    data = su.io.load_json(json_path)\n",
    "\n",
    "    video_dir = config[1]\n",
    "    data_new = []\n",
    "    for d in data:\n",
    "        video_path = f\"{video_dir}/{d['video']}\"\n",
    "\n",
    "        # Hacks\n",
    "        if key == 'Action Antonym':\n",
    "            video_path = video_path.replace(\".avi\", \".mp4\")\n",
    "            d['video'] = d['video'].replace(\".avi\", \".mp4\")\n",
    "\n",
    "        if os.path.exists(video_path):\n",
    "            data_new.append(d)\n",
    "        else:\n",
    "            continue\n",
    "    # print(f\"Number of videos for {key}: \", len(data_new))\n",
    "\n",
    "    for d in su.log.tqdm_iterator(data_new, desc=f\"Filtering video for {key}:\"):\n",
    "        video_path = f\"{video_dir}/{d['video']}\"\n",
    "        assert os.path.exists(video_path)\n",
    "\n",
    "        assert 'question' in d\n",
    "        assert 'candidates' in d\n",
    "        assert 'answer' in d\n",
    "\n",
    "        # if config[-1]:\n",
    "        #     print(d)\n",
    "        d['video_path'] = video_path\n",
    "        d['key'] = key\n",
    "\n",
    "        df.append(d)\n",
    "df = pd.DataFrame(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e0453fa-bcd8-46f8-b4cf-aa1b3be7c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 1,\n",
    "    \"top_p\": 1,\n",
    "    \"temperature\": 0.,\n",
    "    \"use_cache\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ea70af3-5b57-4b36-9ae1-bed9c7b2b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_prompt(messages):\n",
    "    \"\"\"\n",
    "    Convert a list of message dictionaries to a prompt string.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"].upper()\n",
    "        prompt += f\"{role}: \"\n",
    "\n",
    "        content_items = message[\"content\"]\n",
    "        for item in content_items:\n",
    "            if item[\"type\"] == \"video\":\n",
    "                prompt += \"<video>\\n\"\n",
    "            elif item[\"type\"] == \"text\":\n",
    "                prompt += item[\"text\"]\n",
    "        \n",
    "        prompt += \" \"\n",
    "    \n",
    "    prompt += \"ASSISTANT: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer_for_videoqa(encoder, video_path, question, options, n_frames=16, generate_kwargs={}, verbose=False, start=None, end=None):\n",
    "    \"\"\"\n",
    "    Generates an answer for VideoQA.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): video path\n",
    "        question (str): question\n",
    "        options (list[str]): list out all the options\n",
    "        n_frames (int): number of frames\n",
    "        generate_kwargs (dict): additional kwargs for model.generate\n",
    "    \"\"\"\n",
    "    assert os.path.exists(video_path)\n",
    "\n",
    "    # Convert options into a single string\n",
    "    # indexed_options = [f\"{j}: {v}\" for j, v in enumerate(options)]\n",
    "    # option_string = '\\n'.join(indexed_options)\n",
    "\n",
    "    indices = ['A', \"B\", \"C\", \"D\"]\n",
    "    indexed_options = [f\"{j}: {v}\" for j, v in zip(indices, options)]\n",
    "    option_string = ' | '.join(indexed_options)\n",
    "\n",
    "    # option_string = ' | '.join(options)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Carefully watch the video and pay attention to the cause and sequence of events,\"\\\n",
    "                            \" the detail and movement of objects, and the action and pose of persons. Based on\"\\\n",
    "                            \"your observations, select the best option that accurately addresses the question.\\n\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": 8.0,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"\n",
    "                    Question: {question}\n",
    "                    Only give the best option.\n",
    "\n",
    "                    Options: {option_string}\n",
    "\n",
    "                    Strictly pick only the letter coressponding to the best choice.\n",
    "                    \"\"\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Convert into a single string prompt\n",
    "    prompt = convert_to_prompt(messages)\n",
    "\n",
    "    # Prepare video\n",
    "    pixel_values = read_frames_decord(video_path, n_frames, start=start, end=end).unsqueeze(0)\n",
    "    pixel_values = transform_pixel_values(pixel_values)\n",
    "    nframes = pixel_values.shape[1]\n",
    "    to_image = ToPILImage()\n",
    "    batched_frames = []\n",
    "    for batch in pixel_values:\n",
    "        frames = [to_image(v) for v in batch]\n",
    "        batched_frames.append(frames)\n",
    "\n",
    "    # Run through model\n",
    "    for frames in batched_frames:\n",
    "        input_prompt = prompt.replace(\"<video>\", \"<image>\"*len(frames))\n",
    "        if verbose:\n",
    "            print(input_prompt)\n",
    "            print(\"=\" * 60)\n",
    "        input_ids = encoder.processor.get_text_inputs(input_prompt)\n",
    "        frames = encoder.processor.get_pixel_values(frames)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": frames,\n",
    "        }\n",
    "        inputs = {k:v.to(encoder.model.device) for k,v in inputs.items() if v is not None}\n",
    "        outputs = encoder.model.generate(\n",
    "            **inputs,\n",
    "            **generate_kwargs,\n",
    "        )\n",
    "        # print(generate_kwargs)\n",
    "        # print(outputs.shape)\n",
    "        # print(outputs[0][inputs['input_ids'][0].shape[0]:].shape)\n",
    "        output_text = encoder.processor.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "        )\n",
    "        break # Safe to break since it is only a single sample\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad5a622e-760e-41c3-92b8-77714ba26d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({np.nan: None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9eff83b-c523-490c-abf2-80f00db91363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': 'video_8686.mp4',\n",
       " 'question': 'The person makes sets of repeated actions. How many times did the person repeat the action in the last set?',\n",
       " 'candidates': ['2', '3', '5', '4'],\n",
       " 'answer': '4',\n",
       " 'video_path': '/scratch/shared/beegfs/piyush/datasets/TVBench/video/action_count/video_8686.mp4',\n",
       " 'key': 'Action Count',\n",
       " 'is_seq': None,\n",
       " 'question_id': None,\n",
       " 'start': None,\n",
       " 'end': None,\n",
       " 'accurate_start': None,\n",
       " 'accurate_end': None,\n",
       " 'video_length': None}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run inference on a single sample\n",
    "i = 100\n",
    "i = 500\n",
    "i = np.random.randint(len(df))\n",
    "i = 0\n",
    "row = df.iloc[i].to_dict()\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50dd7ee5-8ebb-400f-ad4b-e9db9776aa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based onyour observations, select the best option that accurately addresses the question.\n",
      " USER: <image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image>\n",
      "\n",
      "                    Question: The person makes sets of repeated actions. How many times did the person repeat the action in the last set?\n",
      "                    Only give the best option.\n",
      "\n",
      "                    Options: A: 2 | B: 3 | C: 5 | D: 4\n",
      "\n",
      "                    Strictly pick only the letter coressponding to the best choice.\n",
      "                     ASSISTANT: \n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cla'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer_for_videoqa(encoder, row['video_path'], row['question'], row['candidates'], n_frames=16, generate_kwargs=generate_kwargs, verbose=True, start=row['start'], end=row['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "331b1a1a-8784-40b3-b555-43a9665253c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# su.visualize.show_single_video(row['video_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a8a4325-f462-4073-97af-5f33b11c03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames = su.video.load_frames_linspace(row['video_path'], n=16)\n",
    "# su.visualize.concat_images_with_border(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc01903-13bf-40e5-8bfb-18ad2bb9ee5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc463970-cd33-4846-9df5-ac37de25c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4f70151-af08-409b-9362-3018ed1332e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c78a5aa40584655b00e61174598610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating answers:   0%|          | 0/2405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterator = su.log.tqdm_iterator(range(len(df)), desc='Generating answers')\n",
    "index_to_letter = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "preds = []\n",
    "trues = []\n",
    "for i in iterator:\n",
    "    row = df.iloc[i].to_dict()\n",
    "    pred_option = generate_answer_for_videoqa(encoder, row['video_path'], row['question'], row['candidates'], n_frames=16, generate_kwargs=generate_kwargs, verbose=False, start=row['start'], end=row['end'])\n",
    "    true_option = index_to_letter[row['candidates'].index(row['answer'])]\n",
    "\n",
    "    preds.append(pred_option)\n",
    "    trues.append(true_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7e8b7486-f0f5-4073-a210-77ae6b5cc960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20291060291060292"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(preds) == np.array(trues)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "391a401c-4456-49c9-bab6-ce72e97e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = preds\n",
    "df['true'] = trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce21e0fe-d3fe-41b5-828d-809941b02033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Count 8.21\n",
      "Object Count 16.89\n",
      "Action Sequence 32.27\n",
      "Object Shuffle 18.22\n",
      "Scene Transition 35.68\n",
      "Action Localization 13.12\n",
      "Action Antonym 59.0\n",
      "Unexpected Action 13.41\n",
      "Egocentric Sequence 4.0\n",
      "Moving Direction 5.6\n"
     ]
    }
   ],
   "source": [
    "for k in df.key.unique():\n",
    "    subdf = df[df.key == k]\n",
    "    acc = np.round((subdf['pred'] == subdf['true']).mean() * 100., 2)\n",
    "    print(k, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b38e13-9d82-47e8-b90c-fd67be81347a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6524e6-2526-48b4-9a68-7384a05b68c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c3c9cad-eb5c-4a0c-b8d8-95149f4b58dc",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349c75f0-ce65-41f7-9e5d-932a58eac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NextQA dataset\n",
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/NExTQA\"\n",
    "csv_path = f\"{data_dir}/mc.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0a168-729f-4f3c-965f-7e92eec4f6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3374532c-6a9a-4ca8-bf64-cce9797e8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.video import read_frames_decord\n",
    "from utils.model import transform_pixel_values\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToPILImage,\n",
    ")\n",
    "\n",
    "\n",
    "def convert_to_prompt(messages):\n",
    "    \"\"\"\n",
    "    Convert a list of message dictionaries to a prompt string.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"].upper()\n",
    "        prompt += f\"{role}: \"\n",
    "        \n",
    "        content_items = message[\"content\"]\n",
    "        for item in content_items:\n",
    "            if item[\"type\"] == \"video\":\n",
    "                prompt += \"<video>\\n\"\n",
    "            elif item[\"type\"] == \"text\":\n",
    "                prompt += item[\"text\"]\n",
    "        \n",
    "        prompt += \" \"\n",
    "    \n",
    "    prompt += \"ASSISTANT: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer_for_videoqa(encoder, video_path, question, options, n_frames=16):\n",
    "    \"\"\"\n",
    "    Generates an answer for VideoQA.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): video path\n",
    "        question (str): question\n",
    "        options (list[str]): list out all the options\n",
    "        n_frames (int): number of frames\n",
    "    \"\"\"\n",
    "    assert os.path.exists(video_path)\n",
    "\n",
    "    # Convert options into a single string\n",
    "    indexed_options = [f\"{j}: {v}\" for j, v in enumerate(options)]\n",
    "    option_string = '\\n'.join(indexed_options)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": 8.0,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"Answer the following question by choosing the right option from provided choices. \\n\n",
    "                    Question: {question} \\n\n",
    "                    Options: \\n {option_string}\n",
    "                    \"\"\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Convert into a single string prompt\n",
    "    prompt = convert_to_prompt(messages)\n",
    "\n",
    "    # Prepare video\n",
    "    pixel_values = read_frames_decord(video_path, n_frames).unsqueeze(0)\n",
    "    pixel_values = transform_pixel_values(pixel_values)\n",
    "    nframes = pixel_values.shape[1]\n",
    "    to_image = ToPILImage()\n",
    "    batched_frames = []\n",
    "    for batch in pixel_values:\n",
    "        frames = [to_image(v) for v in batch]\n",
    "        batched_frames.append(frames)\n",
    "\n",
    "    # Run through model\n",
    "    for frames in batched_frames:\n",
    "        input_prompt = prompt.replace(\"<video>\", \"<image>\"*len(frames))\n",
    "        input_ids = encoder.processor.get_text_inputs(input_prompt)\n",
    "        frames = encoder.processor.get_pixel_values(frames)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": frames,\n",
    "        }\n",
    "        inputs = {k:v.to(encoder.model.device) for k,v in inputs.items() if v is not None}\n",
    "        outputs = encoder.model.generate(\n",
    "            **inputs,\n",
    "            **generate_kwargs,\n",
    "        )\n",
    "        output_text = encoder.processor.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "        )\n",
    "        break # Safe to break since it is only a single sample\n",
    "    return output_text, indexed_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf891c89-91d9-4cdb-a3c0-4cb6cc23b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': 9528960175,\n",
       " 'frame_count': 4500,\n",
       " 'width': 640,\n",
       " 'height': 480,\n",
       " 'question': 'where is the child at',\n",
       " 'answer': 3,\n",
       " 'qid': 7,\n",
       " 'type': 'DL',\n",
       " 'a0': 'playground',\n",
       " 'a1': 'along a pathway',\n",
       " 'a2': 'dining table',\n",
       " 'a3': 'bedroom',\n",
       " 'a4': 'bathtub'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4bc867cd-fb47-4ab1-98e8-da50e23efac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keyvalue_markdown(key, value, color='black'):\n",
    "    display(Markdown(f'<span style=\"color:{color}\">{key}</span>: {value}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3890ab6-760d-421b-8913-9e683af709d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0089d38e0e4d44e18b21ee21f5783ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Video ID: 11019529085'), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Question**</span>: where did the man get the microphone from?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Options**\n",
       "</span>: ['laptop', 'lady in skirt', 'man in shirt', 'his teammates', 'microphone stand']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:red\">**Generated text**</span>: Answer: 4: microphone stand"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:limegreen\">**Ground truth**</span>: 4: microphone stand"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a sample\n",
    "# i = 10\n",
    "i = np.random.randint(len(df))\n",
    "row = df.iloc[i].to_dict()\n",
    "\n",
    "video_path = f\"{data_dir}/NExTVideo/{row['video']}.mp4\"\n",
    "assert os.path.exists(video_path)\n",
    "display(su.visualize.show_single_image_sequence(video_path, label=f\"Video ID: {row['video']}\"))\n",
    "\n",
    "question = row['question']\n",
    "\n",
    "# Add question mark\n",
    "if not question.endswith(\"?\"):\n",
    "    question += '?'\n",
    "\n",
    "# Prepare options for MCQ\n",
    "options = [\n",
    "    row['a0'], row['a1'], row['a2'], row['a3'], row['a4'],\n",
    "]\n",
    "\n",
    "show_keyvalue_markdown('**Question**', question, color='blue')\n",
    "show_keyvalue_markdown('**Options**\\n', options, color='blue')\n",
    "# display(Markdown(f'<span style=\"color:blue\">**Question**</span>: {question}'))\n",
    "# display(Markdown(f'**Options**: \\n {options}'))\n",
    "\n",
    "generated_answer, indexed_options = generate_answer_for_videoqa(\n",
    "    encoder=encoder, \n",
    "    video_path=video_path,\n",
    "    question=question,\n",
    "    options=options,\n",
    "    n_frames=16,\n",
    ")\n",
    "# print(generated_answer)\n",
    "# print(indexed_options)\n",
    "\n",
    "show_keyvalue_markdown('**Generated text**', generated_answer, color='red')\n",
    "\n",
    "answer_true = indexed_options[row['answer']]\n",
    "show_keyvalue_markdown('**Ground truth**', answer_true, color='limegreen')\n",
    "\n",
    "# display(Markdown(f\"**Generated text**: {generated_answer}\"))\n",
    "\n",
    "\n",
    "# display(Markdown(f\"**True answer**: {answer_true}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82be6d-80a1-4b0f-bab0-09bf71dd7497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c251d5-3a87-4787-90c3-c1583f95ed96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c69a3-a3aa-4ff6-9a1c-1fe2b2b6d814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681aee4-7c30-44a8-9a63-075d5283f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "206310e1-f603-486b-8ad2-34dde6edbc1a",
   "metadata": {},
   "source": [
    "### Dev code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60312c89-3c7d-4254-ac5c-76497490a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"role\": \"user\",\n",
      "  \"content\": [\n",
      "   {\n",
      "    \"type\": \"video\",\n",
      "    \"video\": \"/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4\",\n",
      "    \"fps\": 8.0\n",
      "   },\n",
      "   {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Answer the following question by choosing the right option from provided choices. \\n\\n                Question: where is the child at? \\n\\n                Options: \\n 0: playground\\n1: along a pathway\\n2: dining table\\n3: bedroom\\n4: bathtub\\n                \"\n",
      "   }\n",
      "  ]\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Messages containing a images list as a video and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": video_path,\n",
    "                \"fps\": 8.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"Answer the following question by choosing the right option from provided choices. \\n\n",
    "                Question: {question} \\n\n",
    "                Options: \\n {option_string}\n",
    "                \"\"\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "import json\n",
    "print(json.dumps(messages, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e9a9f46-8a76-41eb-b969-30b974833e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: <video>\n",
      "Answer the following question by choosing the right option from provided choices. \n",
      "\n",
      "                Question: where is the child at? \n",
      "\n",
      "                Options: \n",
      " 0: playground\n",
      "1: along a pathway\n",
      "2: dining table\n",
      "3: bedroom\n",
      "4: bathtub\n",
      "                 ASSISTANT: \n"
     ]
    }
   ],
   "source": [
    "def convert_to_prompt(messages):\n",
    "    \"\"\"\n",
    "    Convert a list of message dictionaries to a prompt string.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"].upper()\n",
    "        prompt += f\"{role}: \"\n",
    "        \n",
    "        content_items = message[\"content\"]\n",
    "        for item in content_items:\n",
    "            if item[\"type\"] == \"video\":\n",
    "                prompt += \"<video>\\n\"\n",
    "            elif item[\"type\"] == \"text\":\n",
    "                prompt += item[\"text\"]\n",
    "        \n",
    "        prompt += \" \"\n",
    "    \n",
    "    prompt += \"ASSISTANT: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompt = convert_to_prompt(messages)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39f9f7e7-a867-4684-a392-20551da4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3: bedroom\n"
     ]
    }
   ],
   "source": [
    "# Process video\n",
    "from utils.video import read_frames_decord\n",
    "from utils.model import transform_pixel_values\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToPILImage,\n",
    ")\n",
    "\n",
    "n_frames = 16\n",
    "pixel_values = read_frames_decord(video_path, n_frames).unsqueeze(0)\n",
    "pixel_values = transform_pixel_values(pixel_values)\n",
    "nframes = pixel_values.shape[1]\n",
    "to_image = ToPILImage()\n",
    "batched_frames = []\n",
    "for batch in pixel_values:\n",
    "    frames = [to_image(v) for v in batch]\n",
    "    batched_frames.append(frames)\n",
    "\n",
    "for frames in batched_frames:\n",
    "    input_prompt = prompt.replace(\"<video>\", \"<image>\"*len(frames))\n",
    "    input_ids = encoder.processor.get_text_inputs(input_prompt)\n",
    "    frames = encoder.processor.get_pixel_values(frames)\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": frames\n",
    "    }\n",
    "    inputs = {k:v.to(encoder.model.device) for k,v in inputs.items() if v is not None}\n",
    "    outputs = encoder.model.generate(\n",
    "        **inputs,\n",
    "        **generate_kwargs,\n",
    "    )\n",
    "    output_text = encoder.processor.tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "    )\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f12ba-2640-407b-92f3-3cd6b2295b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2aa1a-5a06-412a-913d-288255207995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e16e2-72e0-472d-8b29-a4103ce90d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46895a21-e577-44c3-8cb1-d03d03bb9263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37615213-e53d-4910-a09e-80f7394373b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4\n"
     ]
    }
   ],
   "source": [
    "print(messages[0]['content'][0]['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0896f71-3115-4c04-9818-8d303bf1aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tarsier2.dataset.utils import format_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2dff3b7-29d3-4621-bb83-dbf480475f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(\n",
    "    model, processor, prompt, video_file, generate_kwargs,\n",
    "):\n",
    "    # inputs = processor(prompt, video_file, edit_prompt=True, return_prompt=True)\n",
    "    sample = format_one_sample(video_file, prompt)\n",
    "    print(sample)\n",
    "    batch_data = processor(sample)\n",
    "    print(f\"###Prompt:\\n{get_prompt_from_data_dict(sample)}\")\n",
    "    model_inputs = {}\n",
    "    for k, v in batch_data.items():\n",
    "        if not isinstance(v, torch.Tensor):\n",
    "            continue\n",
    "        model_inputs[k] = v.to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        **generate_kwargs,\n",
    "    )\n",
    "    output_text = processor.processor.tokenizer.decode(\n",
    "        outputs[0][model_inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "    )\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d63bebd-f50a-40bb-97db-8389188ca9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': [{'type': 'video', 'video': {'video_file': '/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4'}}, {'type': 'text', 'text': 'Answer the following question by choosing the right option from provided choices. \\n\\n                Question: where is the child at? \\n\\n                Options: \\n 0: playground\\n1: along a pathway\\n2: dining table\\n3: bedroom\\n4: bathtub\\n                '}]}, {'role': 'assistant', 'content': []}], 'task': 'video/QA'}\n",
      "> \u001b[0;32m/users/piyush/projects/CaReBench/models/tarsier/processor.py\u001b[0m(139)\u001b[0;36mget_text_inputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    138 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 139 \u001b[0;31m        \u001b[0mprompt_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will add <s>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    140 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': [{'type': 'video', 'video': {'video_file': '/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4'}}, {'type': 'text', 'text': 'Answer the following question by choosing the right option from provided choices. \\n\\n                Question: where is the child at? \\n\\n                Options: \\n 0: playground\\n1: along a pathway\\n2: dining table\\n3: bedroom\\n4: bathtub\\n                '}]}, {'role': 'assistant', 'content': []}], 'task': 'video/QA'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.tokenizer.encode(text, add_special_tokens=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.tokenizer.encode('answer this qn', add_special_tokens=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1234, 445, 3855, 29876]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "generate_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"top_p\": 1,\n",
    "    \"temperature\": 0.,\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "input_file = messages[0]['content'][0]['video']\n",
    "prompt = messages[0]['content'][-1]['text']\n",
    "\n",
    "pred = process_one(encoder.model, encoder.processor, prompt, input_file, generate_kwargs)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd6e0f-efd4-4913-9439-20d58b1c5682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2ce97-900b-4753-b9ad-f17c83433561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58727ed9-c087-4a58-bc17-17851e4e86d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf22f9-ced9-4247-bb8f-82358f748689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6cf907-8c23-4c89-a0f5-da4687275f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <video>\\nSummary above video in one word: ASSISTANT: '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.video_eol_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdaaf78-4f19-4390-a934-84bc1b491978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab505b4f-bfbc-4d80-9c86-cedcee2e9430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a37ec-8307-4c43-babf-d37f3a3e0fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba4c623-8205-4fc6-abbf-3a3aca257c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomImageProcessor' object has no attribute 'apply_chat_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m      2\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     }\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Preparation for inference\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m(\n\u001b[1;32m     28\u001b[0m     messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m image_inputs, video_inputs \u001b[38;5;241m=\u001b[39m process_vision_info(messages)\n\u001b[1;32m     31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     32\u001b[0m     text\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m     33\u001b[0m     images\u001b[38;5;241m=\u001b[39mimage_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomImageProcessor' object has no attribute 'apply_chat_template'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparation for inference\n",
    "text = encoder.processor.processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference\n",
    "generated_ids = encoder.model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e45c3e-2748-48fd-92c3-1213d96be4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.tarsier.processor.CustomImageProcessor at 0x7f9e5b505810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.processor.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90feecb-6895-4e43-ba05-de33a1224f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b263ec3-befc-4d0e-adfb-3adde0cae33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261808d-9cb0-427d-a158-1265cda6708b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
