{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9bb1dcae-c6e0-4fa1-b586-d4363f2edf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from utils.video import read_frames_decord\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "import shared.utils as su\n",
    "from notebooks.eval_care_retrieval import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99cc0ebc-8b5c-4b67-b070-59cec872daba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_tokens.json\t\t  model.safetensors.index.json\n",
      "config.json\t\t\t  preprocessor_config.json\n",
      "generation_config.json\t\t  processor_config.json\n",
      "metrics\t\t\t\t  special_tokens_map.json\n",
      "model-00001-of-00003.safetensors  tokenizer.json\n",
      "model-00002-of-00003.safetensors  tokenizer.model\n",
      "model-00003-of-00003.safetensors  tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EncoderForTarsier from /work/piyush/experiments/CaRe/Tarsier-7b/nli-9k+ego4d-1k/merged_checkpoint\n",
      "### do_image_padding is set as False, images will be resized directly!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "TarsierForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e956990de649768c59d327803ae9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::: Number of total parameters in TarsierForConditionalGeneration: 7063.427M\n"
     ]
    }
   ],
   "source": [
    "from models.modeling_encoders import AutoEncoder\n",
    "\n",
    "model_id = \"/work/piyush/experiments/CaRe/Tarsier-7b/nli-9k+ego4d-1k/merged_checkpoint\"\n",
    "!ls $model_id\n",
    "\n",
    "encoder = AutoEncoder.from_pretrained(model_id, device_map='auto')\n",
    "su.misc.num_params(encoder.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349c75f0-ce65-41f7-9e5d-932a58eac1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NextQA dataset\n",
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/NExTQA\"\n",
    "csv_path = f\"{data_dir}/mc.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0a168-729f-4f3c-965f-7e92eec4f6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3374532c-6a9a-4ca8-bf64-cce9797e8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.video import read_frames_decord\n",
    "from utils.model import transform_pixel_values\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToPILImage,\n",
    ")\n",
    "\n",
    "\n",
    "def convert_to_prompt(messages):\n",
    "    \"\"\"\n",
    "    Convert a list of message dictionaries to a prompt string.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"].upper()\n",
    "        prompt += f\"{role}: \"\n",
    "        \n",
    "        content_items = message[\"content\"]\n",
    "        for item in content_items:\n",
    "            if item[\"type\"] == \"video\":\n",
    "                prompt += \"<video>\\n\"\n",
    "            elif item[\"type\"] == \"text\":\n",
    "                prompt += item[\"text\"]\n",
    "        \n",
    "        prompt += \" \"\n",
    "    \n",
    "    prompt += \"ASSISTANT: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer_for_videoqa(encoder, video_path, question, options, n_frames=16):\n",
    "    \"\"\"\n",
    "    Generates an answer for VideoQA.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): video path\n",
    "        question (str): question\n",
    "        options (list[str]): list out all the options\n",
    "        n_frames (int): number of frames\n",
    "    \"\"\"\n",
    "    assert os.path.exists(video_path)\n",
    "\n",
    "    # Convert options into a single string\n",
    "    indexed_options = [f\"{j}: {v}\" for j, v in enumerate(options)]\n",
    "    option_string = '\\n'.join(indexed_options)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"video\",\n",
    "                    \"video\": video_path,\n",
    "                    \"fps\": 8.0,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"Answer the following question by choosing the right option from provided choices. \\n\n",
    "                    Question: {question} \\n\n",
    "                    Options: \\n {option_string}\n",
    "                    \"\"\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Convert into a single string prompt\n",
    "    prompt = convert_to_prompt(messages)\n",
    "\n",
    "    # Prepare video\n",
    "    pixel_values = read_frames_decord(video_path, n_frames).unsqueeze(0)\n",
    "    pixel_values = transform_pixel_values(pixel_values)\n",
    "    nframes = pixel_values.shape[1]\n",
    "    to_image = ToPILImage()\n",
    "    batched_frames = []\n",
    "    for batch in pixel_values:\n",
    "        frames = [to_image(v) for v in batch]\n",
    "        batched_frames.append(frames)\n",
    "\n",
    "    # Run through model\n",
    "    for frames in batched_frames:\n",
    "        input_prompt = prompt.replace(\"<video>\", \"<image>\"*len(frames))\n",
    "        input_ids = encoder.processor.get_text_inputs(input_prompt)\n",
    "        frames = encoder.processor.get_pixel_values(frames)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": frames,\n",
    "        }\n",
    "        inputs = {k:v.to(encoder.model.device) for k,v in inputs.items() if v is not None}\n",
    "        outputs = encoder.model.generate(\n",
    "            **inputs,\n",
    "            **generate_kwargs,\n",
    "        )\n",
    "        output_text = encoder.processor.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "        )\n",
    "        break # Safe to break since it is only a single sample\n",
    "    return output_text, indexed_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf891c89-91d9-4cdb-a3c0-4cb6cc23b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': 9528960175,\n",
       " 'frame_count': 4500,\n",
       " 'width': 640,\n",
       " 'height': 480,\n",
       " 'question': 'where is the child at',\n",
       " 'answer': 3,\n",
       " 'qid': 7,\n",
       " 'type': 'DL',\n",
       " 'a0': 'playground',\n",
       " 'a1': 'along a pathway',\n",
       " 'a2': 'dining table',\n",
       " 'a3': 'bedroom',\n",
       " 'a4': 'bathtub'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4bc867cd-fb47-4ab1-98e8-da50e23efac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keyvalue_markdown(key, value, color='black'):\n",
    "    display(Markdown(f'<span style=\"color:{color}\">{key}</span>: {value}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3890ab6-760d-421b-8913-9e683af709d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0089d38e0e4d44e18b21ee21f5783ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='Video ID: 11019529085'), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Question**</span>: where did the man get the microphone from?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:blue\">**Options**\n",
       "</span>: ['laptop', 'lady in skirt', 'man in shirt', 'his teammates', 'microphone stand']"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:red\">**Generated text**</span>: Answer: 4: microphone stand"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:limegreen\">**Ground truth**</span>: 4: microphone stand"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a sample\n",
    "# i = 10\n",
    "i = np.random.randint(len(df))\n",
    "row = df.iloc[i].to_dict()\n",
    "\n",
    "video_path = f\"{data_dir}/NExTVideo/{row['video']}.mp4\"\n",
    "assert os.path.exists(video_path)\n",
    "display(su.visualize.show_single_image_sequence(video_path, label=f\"Video ID: {row['video']}\"))\n",
    "\n",
    "question = row['question']\n",
    "\n",
    "# Add question mark\n",
    "if not question.endswith(\"?\"):\n",
    "    question += '?'\n",
    "\n",
    "# Prepare options for MCQ\n",
    "options = [\n",
    "    row['a0'], row['a1'], row['a2'], row['a3'], row['a4'],\n",
    "]\n",
    "\n",
    "show_keyvalue_markdown('**Question**', question, color='blue')\n",
    "show_keyvalue_markdown('**Options**\\n', options, color='blue')\n",
    "# display(Markdown(f'<span style=\"color:blue\">**Question**</span>: {question}'))\n",
    "# display(Markdown(f'**Options**: \\n {options}'))\n",
    "\n",
    "generated_answer, indexed_options = generate_answer_for_videoqa(\n",
    "    encoder=encoder, \n",
    "    video_path=video_path,\n",
    "    question=question,\n",
    "    options=options,\n",
    "    n_frames=16,\n",
    ")\n",
    "# print(generated_answer)\n",
    "# print(indexed_options)\n",
    "\n",
    "show_keyvalue_markdown('**Generated text**', generated_answer, color='red')\n",
    "\n",
    "answer_true = indexed_options[row['answer']]\n",
    "show_keyvalue_markdown('**Ground truth**', answer_true, color='limegreen')\n",
    "\n",
    "# display(Markdown(f\"**Generated text**: {generated_answer}\"))\n",
    "\n",
    "\n",
    "# display(Markdown(f\"**True answer**: {answer_true}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82be6d-80a1-4b0f-bab0-09bf71dd7497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c251d5-3a87-4787-90c3-c1583f95ed96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c69a3-a3aa-4ff6-9a1c-1fe2b2b6d814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681aee4-7c30-44a8-9a63-075d5283f0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "206310e1-f603-486b-8ad2-34dde6edbc1a",
   "metadata": {},
   "source": [
    "### Dev code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60312c89-3c7d-4254-ac5c-76497490a47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"role\": \"user\",\n",
      "  \"content\": [\n",
      "   {\n",
      "    \"type\": \"video\",\n",
      "    \"video\": \"/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4\",\n",
      "    \"fps\": 8.0\n",
      "   },\n",
      "   {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Answer the following question by choosing the right option from provided choices. \\n\\n                Question: where is the child at? \\n\\n                Options: \\n 0: playground\\n1: along a pathway\\n2: dining table\\n3: bedroom\\n4: bathtub\\n                \"\n",
      "   }\n",
      "  ]\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Messages containing a images list as a video and a text query\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": video_path,\n",
    "                \"fps\": 8.0,\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"Answer the following question by choosing the right option from provided choices. \\n\n",
    "                Question: {question} \\n\n",
    "                Options: \\n {option_string}\n",
    "                \"\"\"\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "import json\n",
    "print(json.dumps(messages, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e9a9f46-8a76-41eb-b969-30b974833e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: <video>\n",
      "Answer the following question by choosing the right option from provided choices. \n",
      "\n",
      "                Question: where is the child at? \n",
      "\n",
      "                Options: \n",
      " 0: playground\n",
      "1: along a pathway\n",
      "2: dining table\n",
      "3: bedroom\n",
      "4: bathtub\n",
      "                 ASSISTANT: \n"
     ]
    }
   ],
   "source": [
    "def convert_to_prompt(messages):\n",
    "    \"\"\"\n",
    "    Convert a list of message dictionaries to a prompt string.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of message dictionaries with 'role' and 'content' fields\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"].upper()\n",
    "        prompt += f\"{role}: \"\n",
    "        \n",
    "        content_items = message[\"content\"]\n",
    "        for item in content_items:\n",
    "            if item[\"type\"] == \"video\":\n",
    "                prompt += \"<video>\\n\"\n",
    "            elif item[\"type\"] == \"text\":\n",
    "                prompt += item[\"text\"]\n",
    "        \n",
    "        prompt += \" \"\n",
    "    \n",
    "    prompt += \"ASSISTANT: \"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "prompt = convert_to_prompt(messages)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39f9f7e7-a867-4684-a392-20551da4d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3: bedroom\n"
     ]
    }
   ],
   "source": [
    "# Process video\n",
    "from utils.video import read_frames_decord\n",
    "from utils.model import transform_pixel_values\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToPILImage,\n",
    ")\n",
    "\n",
    "n_frames = 16\n",
    "pixel_values = read_frames_decord(video_path, n_frames).unsqueeze(0)\n",
    "pixel_values = transform_pixel_values(pixel_values)\n",
    "nframes = pixel_values.shape[1]\n",
    "to_image = ToPILImage()\n",
    "batched_frames = []\n",
    "for batch in pixel_values:\n",
    "    frames = [to_image(v) for v in batch]\n",
    "    batched_frames.append(frames)\n",
    "\n",
    "for frames in batched_frames:\n",
    "    input_prompt = prompt.replace(\"<video>\", \"<image>\"*len(frames))\n",
    "    input_ids = encoder.processor.get_text_inputs(input_prompt)\n",
    "    frames = encoder.processor.get_pixel_values(frames)\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": frames\n",
    "    }\n",
    "    inputs = {k:v.to(encoder.model.device) for k,v in inputs.items() if v is not None}\n",
    "    outputs = encoder.model.generate(\n",
    "        **inputs,\n",
    "        **generate_kwargs,\n",
    "    )\n",
    "    output_text = encoder.processor.tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "    )\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7f12ba-2640-407b-92f3-3cd6b2295b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f2aa1a-5a06-412a-913d-288255207995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e16e2-72e0-472d-8b29-a4103ce90d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46895a21-e577-44c3-8cb1-d03d03bb9263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37615213-e53d-4910-a09e-80f7394373b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4\n"
     ]
    }
   ],
   "source": [
    "print(messages[0]['content'][0]['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0896f71-3115-4c04-9818-8d303bf1aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.tarsier2.dataset.utils import format_one_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2dff3b7-29d3-4621-bb83-dbf480475f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(\n",
    "    model, processor, prompt, video_file, generate_kwargs,\n",
    "):\n",
    "    # inputs = processor(prompt, video_file, edit_prompt=True, return_prompt=True)\n",
    "    sample = format_one_sample(video_file, prompt)\n",
    "    print(sample)\n",
    "    batch_data = processor(sample)\n",
    "    print(f\"###Prompt:\\n{get_prompt_from_data_dict(sample)}\")\n",
    "    model_inputs = {}\n",
    "    for k, v in batch_data.items():\n",
    "        if not isinstance(v, torch.Tensor):\n",
    "            continue\n",
    "        model_inputs[k] = v.to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        **generate_kwargs,\n",
    "    )\n",
    "    output_text = processor.processor.tokenizer.decode(\n",
    "        outputs[0][model_inputs['input_ids'][0].shape[0]:], skip_special_tokens=True,\n",
    "    )\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d63bebd-f50a-40bb-97db-8389188ca9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': [{'type': 'video', 'video': {'video_file': '/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4'}}, {'type': 'text', 'text': 'Answer the following question by choosing the right option from provided choices. \\n\\n                Question: where is the child at? \\n\\n                Options: \\n 0: playground\\n1: along a pathway\\n2: dining table\\n3: bedroom\\n4: bathtub\\n                '}]}, {'role': 'assistant', 'content': []}], 'task': 'video/QA'}\n",
      "> \u001b[0;32m/users/piyush/projects/CaReBench/models/tarsier/processor.py\u001b[0m(139)\u001b[0;36mget_text_inputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    138 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 139 \u001b[0;31m        \u001b[0mprompt_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will add <s>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    140 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'role': 'user', 'content': [{'type': 'video', 'video': {'video_file': '/scratch/shared/beegfs/piyush/datasets/NExTQA/NExTVideo/9528960175.mp4'}}, {'type': 'text', 'text': 'Answer the following question by choosing the right option from provided choices. \\n\\n                Question: where is the child at? \\n\\n                Options: \\n 0: playground\\n1: along a pathway\\n2: dining table\\n3: bedroom\\n4: bathtub\\n                '}]}, {'role': 'assistant', 'content': []}], 'task': 'video/QA'}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.tokenizer.encode(text, add_special_tokens=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  self.tokenizer.encode('answer this qn', add_special_tokens=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1234, 445, 3855, 29876]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "generate_kwargs = {\n",
    "    \"do_sample\": False,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"top_p\": 1,\n",
    "    \"temperature\": 0.,\n",
    "    \"use_cache\": True,\n",
    "}\n",
    "input_file = messages[0]['content'][0]['video']\n",
    "prompt = messages[0]['content'][-1]['text']\n",
    "\n",
    "pred = process_one(encoder.model, encoder.processor, prompt, input_file, generate_kwargs)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd6e0f-efd4-4913-9439-20d58b1c5682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2ce97-900b-4753-b9ad-f17c83433561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58727ed9-c087-4a58-bc17-17851e4e86d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf22f9-ced9-4247-bb8f-82358f748689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6cf907-8c23-4c89-a0f5-da4687275f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER: <video>\\nSummary above video in one word: ASSISTANT: '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.video_eol_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdaaf78-4f19-4390-a934-84bc1b491978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab505b4f-bfbc-4d80-9c86-cedcee2e9430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a37ec-8307-4c43-babf-d37f3a3e0fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba4c623-8205-4fc6-abbf-3a3aca257c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomImageProcessor' object has no attribute 'apply_chat_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m\n\u001b[1;32m      2\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     }\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Preparation for inference\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m(\n\u001b[1;32m     28\u001b[0m     messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m image_inputs, video_inputs \u001b[38;5;241m=\u001b[39m process_vision_info(messages)\n\u001b[1;32m     31\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     32\u001b[0m     text\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m     33\u001b[0m     images\u001b[38;5;241m=\u001b[39mimage_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomImageProcessor' object has no attribute 'apply_chat_template'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preparation for inference\n",
    "text = encoder.processor.processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference\n",
    "generated_ids = encoder.model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e45c3e-2748-48fd-92c3-1213d96be4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<models.tarsier.processor.CustomImageProcessor at 0x7f9e5b505810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.processor.processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90feecb-6895-4e43-ba05-de33a1224f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b263ec3-befc-4d0e-adfb-3adde0cae33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261808d-9cb0-427d-a158-1265cda6708b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
