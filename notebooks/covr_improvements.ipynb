{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65933529-2d19-44b3-a842-cf67714dd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"False\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import decord\n",
    "import json\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from utils.video import read_frames_decord\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "import shared.utils as su\n",
    "from notebooks.eval_care_retrieval import load_model\n",
    "from utils.video import read_frames_decord\n",
    "from utils.model import transform_pixel_values\n",
    "from torchvision.transforms.v2 import (\n",
    "    ToPILImage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a375d70a-ad6a-4d2b-ae23-69fda1f99724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in CoVR-test:  2556\n",
      "Number of rows with all videos available:  (2556, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'txt1': 'Digital network, white lines and dots',\n",
       " 'txt2': 'Digital network, green lines and dots',\n",
       " 'sim_txt': 0.8944025,\n",
       " 'pth1': '112/1016223889',\n",
       " 'pth2': '112/1016223877',\n",
       " 'edit': 'replace the white lines and dots with green',\n",
       " 'scores': '[0.3524, 0.4087, 0.4136, 0.4111, 0.4137, 0.4084, 0.4062, 0.4054, 0.4025, 0.413, 0.4115, 0.4105, 0.4123, 0.4099, 0.4107]',\n",
       " 'video1': '070651_070700/1016223889.mp4',\n",
       " 'video2': '032701_032750/1016223877.mp4',\n",
       " 'video1_path': '/datasets/WebVid/videos/070651_070700/1016223889.mp4',\n",
       " 'video2_path': '/datasets/WebVid/videos/032701_032750/1016223877.mp4'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/scratch/shared/beegfs/piyush/datasets/WebVid-CoVR\"\n",
    "video_dir = '/datasets/WebVid/videos'\n",
    "\n",
    "df = pd.read_csv(f\"{data_dir}/webvid8m-covr_test-cleaned.csv\")\n",
    "print(\"Number of rows in CoVR-test: \", len(df))\n",
    "\n",
    "df['video1_path'] = df['video1'].apply(lambda x: f\"{video_dir}/{x}\")\n",
    "df['video2_path'] = df['video2'].apply(lambda x: f\"{video_dir}/{x}\")\n",
    "df = df[df.video1_path.apply(os.path.exists) & df.video2_path.apply(os.path.exists)]\n",
    "print(\"Number of rows with all videos available: \", df.shape)\n",
    "\n",
    "d = set(df['video1_path']).intersection(set(df['video2_path']))\n",
    "\n",
    "# Remove problematic videos\n",
    "df = df[df.video1_path != '/datasets/WebVid/videos/108401_108450/6507308.mp4']\n",
    "\n",
    "df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a8b5b6-89be-4929-9c6d-8979ae60532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2555"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings_single_frame = su.io.load_pkl(\n",
    "    f\"{data_dir}/query_embeddings_single_frame.pkl\"\n",
    ")\n",
    "len(query_embeddings_single_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ffddcb9-6ab8-43ab-a754-90bf2fdaf4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2555"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings_single_frame = su.io.load_pkl(\n",
    "    f\"{data_dir}/query_embeddings_single_frame_with_gen_caption.pkl\"\n",
    ")\n",
    "len(query_embeddings_single_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12059407-7c06-4792-a143-8cb1e6037649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2555"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_embeddings = su.io.load_pkl(\n",
    "    f\"{data_dir}/gallery_embeddings-nframes_15.pkl\"\n",
    ")\n",
    "len(candidate_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc625436-c81f-4429-807d-2ca585fb4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2555, 4096) (2555, 4096)\n",
      "{\n",
      "  \"txt_r1\": 49.19765166340509,\n",
      "  \"txt_r5\": 74.12915851272015,\n",
      "  \"txt_r10\": 82.15264187866927,\n",
      "  \"txt_r_mean\": 68.4931506849315,\n",
      "  \"txt_r50\": 95.22504892367905\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def gather_metrics(query_embeds, candidates):\n",
    "    \n",
    "    from utils.general_retrieval_metrics import itm_eval\n",
    "    \n",
    "    zq = []\n",
    "    zc = []\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i].to_dict()\n",
    "        query_key = f\"{row['edit']}|{row['video1']}\"\n",
    "        candi_key = row['video2']\n",
    "        if query_key not in query_embeds or candi_key not in candidates:\n",
    "            print(f\"Missing value for {i}. Skipped.\")\n",
    "            continue\n",
    "        zq.append(query_embeds[query_key])\n",
    "        zc.append(candidates[candi_key])\n",
    "    zq = torch.stack(zq).numpy()\n",
    "    zc = torch.stack(zc).numpy()\n",
    "    print(zq.shape, zc.shape)\n",
    "    \n",
    "    # i:q and t:c; and we care about q2c metrics, i.e., i2t, i.e., text_*\n",
    "    score_q2c = zq @ zc.T\n",
    "    score_c2q = zc @ zq.T\n",
    "    indices = {i:i for i in range(len(score_q2c))}\n",
    "    metrics = itm_eval(scores_i2t=score_q2c, scores_t2i=score_c2q, txt2img=indices, img2txt=indices, add_50=True)\n",
    "\n",
    "    metrics = {k: v for k, v in metrics.items() if 'txt' in k}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "metrics_single_frame = gather_metrics(query_embeddings_single_frame, candidate_embeddings)\n",
    "print(json.dumps(metrics_single_frame, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf2bca-57fb-4694-8344-efc96d24fd77",
   "metadata": {},
   "source": [
    "#### BLIP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2340258-6618-4511-be9d-f812314ba88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a11b988015b4df8ab0c1067cb6bcfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2555 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2555"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_candidate_embeddings(feat_dir):\n",
    "    embeds = {}\n",
    "    video_ids = df.video2.tolist()\n",
    "    paths = df.video2.apply(lambda x: f\"{feat_dir}/{x.split('.mp4')[0]}.pth\").tolist()\n",
    "    for i in su.log.tqdm_iterator(range(len(video_ids))):\n",
    "        embeds[video_ids[i]] = torch.load(paths[i])\n",
    "    return embeds\n",
    "\n",
    "candidate_embeddings_blip2 = load_candidate_embeddings(f\"{data_dir}/blip2-vid-embs-large-all\")\n",
    "len(candidate_embeddings_blip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b24641d-3f74-463d-9a56-e5e7f9026a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 32, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_embeddings_blip2['032701_032750/1016223877.mp4'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a9d8d-b140-46b9-8401-049a69a95bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a6f68-a99f-4541-8adc-b72b465ee742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be102e67-19a1-422f-b763-fd14d726b699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
